# M3: Small LSTM + Additive (Bahdanau) Attention
# Expected parameters: ~380K
# Expected training time: ~3h

model:
  name: "M3_Small_Additive_Attention"
  type: "additive_attention"

  # Architecture
  input_size: 5
  hidden_size: 64
  num_layers: 3
  output_size: 1

  # Attention configuration
  attention: "additive"     # Bahdanau: score_ij = v^T * tanh(W * h_i + U * h_j)
  attention_size: 128       # Dimension of attention layer

training:
  learning_rate: 0.001
