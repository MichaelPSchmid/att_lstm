# M4: Small LSTM + Simple Attention (WITH DROPOUT - Ablation Study)
# Expected parameters: ~350K
# Expected training time: ~3h

model:
  name: "M4_Small_Simple_Attention_Dropout"
  type: "simple_attention"

  # Architecture
  input_size: 5
  hidden_size: 64
  num_layers: 3
  output_size: 1

  # Attention configuration
  attention: "simple"       # Linear attention: score_i = W * h_i + b

training:
  learning_rate: 0.001
  dropout: 0.2              # Ablation: test with dropout
  early_stopping:
    patience: 15            # More patience needed with dropout

attention:
  enabled: true             # Save attention weights during training
