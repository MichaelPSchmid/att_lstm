{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Weight Analysis\n",
    "\n",
    "**Ziel:** Untersuchen, warum Attention-Mechanismen keinen Mehrwert gegenüber der LSTM-Baseline liefern.\n",
    "\n",
    "**Modelle:**\n",
    "- M4: Small LSTM + Simple Attention (64 hidden, 3L)\n",
    "- M6: Medium LSTM + Simple Attention (128 hidden, 5L)\n",
    "- M7: Medium LSTM + Additive Attention (128 hidden, 5L)\n",
    "- M8: Medium LSTM + Scaled Dot-Product Attention (128 hidden, 5L)\n",
    "\n",
    "**Analysen:**\n",
    "1. Entropie — Wie scharf/uniform ist die Attention?\n",
    "2. Temporales Profil — Wohin fokussiert die Attention?\n",
    "3. Konvergenz über Epochen — Lernt die Attention etwas?\n",
    "4. Cross-Seed Konsistenz — Sind Patterns stabil?\n",
    "5. Modellvergleich — Qualitative Unterschiede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from pathlib import Path\n",
    "from scipy.stats import entropy as scipy_entropy\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (14, 6),\n",
    "    'font.size': 11,\n",
    "    'axes.titlesize': 13,\n",
    "    'axes.labelsize': 11,\n",
    "    'legend.fontsize': 9,\n",
    "})\n",
    "\n",
    "BASE_DIR = Path('../attention_weights')\n",
    "SEEDS = [7, 42, 94, 123, 231]\n",
    "N_TIMESTEPS = 50\n",
    "MAX_ENTROPY = np.log(N_TIMESTEPS)  # uniform distribution entropy\n",
    "\n",
    "MODELS = {\n",
    "    'M4': {'prefix': 'M4_Small_Simple_Attention', 'label': 'M4 Small Simple', 'color': '#2196F3'},\n",
    "    'M6': {'prefix': 'M6_Medium_Simple_Attention', 'label': 'M6 Medium Simple', 'color': '#4CAF50'},\n",
    "    'M7': {'prefix': 'M7_Medium_Additive_Attention', 'label': 'M7 Medium Additive', 'color': '#FF9800'},\n",
    "    'M8': {'prefix': 'M8_Medium_Scaled_DP_Attention', 'label': 'M8 Medium Scaled DP', 'color': '#F44336'},\n",
    "}\n",
    "\n",
    "print(f'Max entropy (uniform over {N_TIMESTEPS} steps): {MAX_ENTROPY:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_epochs(model_key: str, seed: int) -> dict[int, np.ndarray]:\n",
    "    \"\"\"Load all epoch attention weights for a model/seed combination.\"\"\"\n",
    "    dirname = f\"{MODELS[model_key]['prefix']}_seed{seed}\"\n",
    "    dirpath = BASE_DIR / dirname\n",
    "    if not dirpath.exists():\n",
    "        return {}\n",
    "    epochs = {}\n",
    "    for f in sorted(dirpath.glob('attention_epoch_*.npy')):\n",
    "        epoch_num = int(f.stem.split('_')[-1])\n",
    "        epochs[epoch_num] = np.load(f)\n",
    "    return epochs\n",
    "\n",
    "\n",
    "def load_test_weights(model_key: str, seed: int) -> np.ndarray | None:\n",
    "    \"\"\"Load test-time attention weights if available.\"\"\"\n",
    "    dirname = f\"{MODELS[model_key]['prefix']}_seed{seed}\"\n",
    "    fpath = BASE_DIR / dirname / 'attention_test.npy'\n",
    "    if fpath.exists():\n",
    "        return np.load(fpath)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_final_weights(model_key: str, seed: int) -> np.ndarray | None:\n",
    "    \"\"\"Get final epoch attention weights (test if available, else last epoch).\"\"\"\n",
    "    test = load_test_weights(model_key, seed)\n",
    "    if test is not None:\n",
    "        return test\n",
    "    epochs = load_all_epochs(model_key, seed)\n",
    "    if epochs:\n",
    "        return epochs[max(epochs.keys())]\n",
    "    return None\n",
    "\n",
    "\n",
    "def normalized_entropy(weights: np.ndarray) -> float:\n",
    "    \"\"\"Compute normalized entropy (0=peaked, 1=uniform).\"\"\"\n",
    "    w = np.clip(weights, 1e-12, None)\n",
    "    w = w / w.sum()\n",
    "    return scipy_entropy(w) / MAX_ENTROPY\n",
    "\n",
    "\n",
    "# Load everything\n",
    "all_data = {}\n",
    "for mk in MODELS:\n",
    "    all_data[mk] = {}\n",
    "    for seed in SEEDS:\n",
    "        all_data[mk][seed] = {\n",
    "            'epochs': load_all_epochs(mk, seed),\n",
    "            'test': load_test_weights(mk, seed),\n",
    "            'final': get_final_weights(mk, seed),\n",
    "        }\n",
    "\n",
    "# Summary\n",
    "for mk in MODELS:\n",
    "    for seed in SEEDS:\n",
    "        n_ep = len(all_data[mk][seed]['epochs'])\n",
    "        has_test = all_data[mk][seed]['test'] is not None\n",
    "        print(f\"{MODELS[mk]['label']:30s} seed={seed:3d}  epochs={n_ep:3d}  test={'yes' if has_test else 'no'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Entropie-Analyse\n",
    "\n",
    "Normierte Entropie: 0 = Attention fokussiert auf einen einzigen Zeitschritt, 1 = komplett uniform (= nutzlos, Attention ignoriert).\n",
    "\n",
    "Wenn die Entropie nahe 1 liegt, gewichtet die Attention alle Zeitschritte gleich und liefert damit keinen Informationsgewinn gegenüber einem einfachen Mittelwert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute normalized entropy for final weights across all seeds\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Bar chart of mean entropy per model\n",
    "ax = axes[0]\n",
    "model_keys = list(MODELS.keys())\n",
    "entropies_per_model = {}\n",
    "for mk in model_keys:\n",
    "    ents = []\n",
    "    for seed in SEEDS:\n",
    "        w = all_data[mk][seed]['final']\n",
    "        if w is not None:\n",
    "            ents.append(normalized_entropy(w))\n",
    "    entropies_per_model[mk] = ents\n",
    "\n",
    "means = [np.mean(entropies_per_model[mk]) for mk in model_keys]\n",
    "stds = [np.std(entropies_per_model[mk]) for mk in model_keys]\n",
    "colors = [MODELS[mk]['color'] for mk in model_keys]\n",
    "labels = [MODELS[mk]['label'] for mk in model_keys]\n",
    "\n",
    "bars = ax.bar(labels, means, yerr=stds, color=colors, alpha=0.8, capsize=5, edgecolor='black', linewidth=0.5)\n",
    "ax.axhline(y=1.0, color='gray', linestyle='--', linewidth=1, label='Uniform (max entropy)')\n",
    "ax.set_ylabel('Normierte Entropie')\n",
    "ax.set_title('Entropie der finalen Attention Weights')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, m, s in zip(bars, means, stds):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + s + 0.02,\n",
    "            f'{m:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Right: Per-seed scatter\n",
    "ax = axes[1]\n",
    "for i, mk in enumerate(model_keys):\n",
    "    for j, seed in enumerate(SEEDS):\n",
    "        w = all_data[mk][seed]['final']\n",
    "        if w is not None:\n",
    "            e = normalized_entropy(w)\n",
    "            ax.scatter(i + (j - 2) * 0.08, e, color=MODELS[mk]['color'],\n",
    "                       s=60, edgecolors='black', linewidth=0.5, zorder=3)\n",
    "\n",
    "ax.axhline(y=1.0, color='gray', linestyle='--', linewidth=1, label='Uniform')\n",
    "ax.set_xticks(range(len(model_keys)))\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_ylabel('Normierte Entropie')\n",
    "ax.set_title('Entropie pro Seed')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print table\n",
    "print(f\"{'Modell':30s} {'Mean':>8s} {'Std':>8s} {'Min':>8s} {'Max':>8s}\")\n",
    "print('-' * 64)\n",
    "for mk in model_keys:\n",
    "    ents = entropies_per_model[mk]\n",
    "    print(f\"{MODELS[mk]['label']:30s} {np.mean(ents):8.4f} {np.std(ents):8.4f} {np.min(ents):8.4f} {np.max(ents):8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Temporales Profil\n",
    "\n",
    "Wo im 50-Schritt-Window liegt der Attention-Fokus? Zeitschritt 0 = ältester, Zeitschritt 49 = aktuellster.\n",
    "\n",
    "Hypothese: Wenn die Attention hauptsächlich auf die letzten Schritte fokussiert, ist sie redundant zum LSTM-Hidden-State (der ohnehin den letzten Zeitschritt am stärksten repräsentiert)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for idx, mk in enumerate(model_keys):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Collect final weights across seeds\n",
    "    all_weights = []\n",
    "    for seed in SEEDS:\n",
    "        w = all_data[mk][seed]['final']\n",
    "        if w is not None:\n",
    "            # Normalize to sum to 1\n",
    "            w_norm = w / w.sum()\n",
    "            all_weights.append(w_norm)\n",
    "            ax.plot(range(N_TIMESTEPS), w_norm, alpha=0.3, color=MODELS[mk]['color'], linewidth=1)\n",
    "    \n",
    "    if all_weights:\n",
    "        mean_w = np.mean(all_weights, axis=0)\n",
    "        std_w = np.std(all_weights, axis=0)\n",
    "        ax.plot(range(N_TIMESTEPS), mean_w, color=MODELS[mk]['color'], linewidth=2.5, label='Mean')\n",
    "        ax.fill_between(range(N_TIMESTEPS), mean_w - std_w, mean_w + std_w,\n",
    "                        alpha=0.2, color=MODELS[mk]['color'])\n",
    "    \n",
    "    # Uniform reference\n",
    "    ax.axhline(y=1/N_TIMESTEPS, color='gray', linestyle='--', linewidth=1, label=f'Uniform (1/{N_TIMESTEPS})')\n",
    "    \n",
    "    ax.set_title(f\"{MODELS[mk]['label']}\")\n",
    "    ax.set_xlabel('Zeitschritt (0=ältester, 49=neuester)')\n",
    "    ax.set_ylabel('Attention Weight')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_xlim(0, 49)\n",
    "\n",
    "plt.suptitle('Temporales Attention-Profil (finale Weights, alle Seeds)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative: How much weight is in the last 5, 10, 20 timesteps?\n",
    "print(f\"{'Modell':30s} {'Last 5':>10s} {'Last 10':>10s} {'Last 20':>10s} {'Peak Pos':>10s}\")\n",
    "print('-' * 75)\n",
    "\n",
    "for mk in model_keys:\n",
    "    last5, last10, last20, peaks = [], [], [], []\n",
    "    for seed in SEEDS:\n",
    "        w = all_data[mk][seed]['final']\n",
    "        if w is not None:\n",
    "            w_norm = w / w.sum()\n",
    "            last5.append(w_norm[-5:].sum())\n",
    "            last10.append(w_norm[-10:].sum())\n",
    "            last20.append(w_norm[-20:].sum())\n",
    "            peaks.append(np.argmax(w_norm))\n",
    "    \n",
    "    print(f\"{MODELS[mk]['label']:30s} \"\n",
    "          f\"{np.mean(last5)*100:9.1f}% \"\n",
    "          f\"{np.mean(last10)*100:9.1f}% \"\n",
    "          f\"{np.mean(last20)*100:9.1f}% \"\n",
    "          f\"{np.mean(peaks):9.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Konvergenz über Epochen\n",
    "\n",
    "Wie entwickelt sich die Attention über das Training? Konvergiert sie zu einem stabilen, informativen Pattern — oder kollabiert sie zu einer uniformen Verteilung?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for idx, mk in enumerate(model_keys):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    for seed in SEEDS:\n",
    "        epochs_data = all_data[mk][seed]['epochs']\n",
    "        if not epochs_data:\n",
    "            continue\n",
    "        ep_nums = sorted(epochs_data.keys())\n",
    "        ent_vals = [normalized_entropy(epochs_data[e]) for e in ep_nums]\n",
    "        ax.plot(ep_nums, ent_vals, marker='.', markersize=3, alpha=0.7,\n",
    "                label=f'Seed {seed}', linewidth=1.2)\n",
    "    \n",
    "    ax.axhline(y=1.0, color='gray', linestyle='--', linewidth=1, label='Uniform')\n",
    "    ax.set_title(f\"{MODELS[mk]['label']}\")\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Normierte Entropie')\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.legend(fontsize=8, ncol=2)\n",
    "\n",
    "plt.suptitle('Entropie-Verlauf über Epochen (pro Seed)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmaps: Attention profile evolution over epochs (seed 42 as representative)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "REPR_SEED = 42\n",
    "\n",
    "for idx, mk in enumerate(model_keys):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    epochs_data = all_data[mk][REPR_SEED]['epochs']\n",
    "    \n",
    "    if not epochs_data:\n",
    "        ax.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(f\"{MODELS[mk]['label']} (seed {REPR_SEED})\")\n",
    "        continue\n",
    "    \n",
    "    ep_nums = sorted(epochs_data.keys())\n",
    "    matrix = np.array([epochs_data[e] / epochs_data[e].sum() for e in ep_nums])\n",
    "    \n",
    "    im = ax.imshow(matrix, aspect='auto', cmap='hot', interpolation='nearest',\n",
    "                   extent=[0, N_TIMESTEPS-1, ep_nums[-1], ep_nums[0]])\n",
    "    ax.set_title(f\"{MODELS[mk]['label']} (seed {REPR_SEED})\")\n",
    "    ax.set_xlabel('Zeitschritt')\n",
    "    ax.set_ylabel('Epoch')\n",
    "    plt.colorbar(im, ax=ax, label='Weight', shrink=0.8)\n",
    "\n",
    "plt.suptitle('Attention-Profil über Epochen (Heatmap)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Seed Konsistenz\n",
    "\n",
    "Wenn die Attention ein robustes Signal lernt, sollten die Patterns über verschiedene Seeds hinweg konsistent sein. Hohe Varianz = die Attention lernt keine stabile Repräsentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "cosine_sims = {}\n",
    "\n",
    "for idx, mk in enumerate(model_keys):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Collect final weights per seed\n",
    "    seed_weights = {}\n",
    "    for seed in SEEDS:\n",
    "        w = all_data[mk][seed]['final']\n",
    "        if w is not None:\n",
    "            seed_weights[seed] = w / w.sum()\n",
    "    \n",
    "    if len(seed_weights) < 2:\n",
    "        continue\n",
    "    \n",
    "    # Pairwise cosine similarity matrix\n",
    "    seeds_list = sorted(seed_weights.keys())\n",
    "    n = len(seeds_list)\n",
    "    sim_matrix = np.ones((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            wi = seed_weights[seeds_list[i]]\n",
    "            wj = seed_weights[seeds_list[j]]\n",
    "            cos_sim = np.dot(wi, wj) / (np.linalg.norm(wi) * np.linalg.norm(wj))\n",
    "            sim_matrix[i, j] = cos_sim\n",
    "            sim_matrix[j, i] = cos_sim\n",
    "    \n",
    "    # Store for summary\n",
    "    upper_tri = sim_matrix[np.triu_indices(n, k=1)]\n",
    "    cosine_sims[mk] = upper_tri\n",
    "    \n",
    "    im = ax.imshow(sim_matrix, cmap='RdYlGn', vmin=0.5, vmax=1.0, interpolation='nearest')\n",
    "    ax.set_xticks(range(n))\n",
    "    ax.set_xticklabels([f'S{s}' for s in seeds_list])\n",
    "    ax.set_yticks(range(n))\n",
    "    ax.set_yticklabels([f'S{s}' for s in seeds_list])\n",
    "    ax.set_title(f\"{MODELS[mk]['label']}\\nmean cos_sim={np.mean(upper_tri):.4f}\")\n",
    "    plt.colorbar(im, ax=ax, label='Cosine Similarity', shrink=0.8)\n",
    "    \n",
    "    # Annotate cells\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ax.text(j, i, f'{sim_matrix[i,j]:.3f}', ha='center', va='center', fontsize=9,\n",
    "                    color='white' if sim_matrix[i,j] < 0.75 else 'black')\n",
    "\n",
    "plt.suptitle('Cross-Seed Konsistenz (Cosine Similarity der finalen Weights)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'Modell':30s} {'Mean CosSim':>12s} {'Std':>8s} {'Min':>8s}\")\n",
    "print('-' * 60)\n",
    "for mk in model_keys:\n",
    "    if mk in cosine_sims:\n",
    "        sims = cosine_sims[mk]\n",
    "        print(f\"{MODELS[mk]['label']:30s} {np.mean(sims):12.4f} {np.std(sims):8.4f} {np.min(sims):8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient of Variation per timestep across seeds\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for idx, mk in enumerate(model_keys):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    weights_matrix = []\n",
    "    for seed in SEEDS:\n",
    "        w = all_data[mk][seed]['final']\n",
    "        if w is not None:\n",
    "            weights_matrix.append(w / w.sum())\n",
    "    \n",
    "    if len(weights_matrix) < 2:\n",
    "        continue\n",
    "    \n",
    "    weights_matrix = np.array(weights_matrix)\n",
    "    mean_per_step = weights_matrix.mean(axis=0)\n",
    "    std_per_step = weights_matrix.std(axis=0)\n",
    "    cv = std_per_step / (mean_per_step + 1e-12)\n",
    "    \n",
    "    ax.bar(range(N_TIMESTEPS), cv, color=MODELS[mk]['color'], alpha=0.7, edgecolor='black', linewidth=0.3)\n",
    "    ax.set_title(f\"{MODELS[mk]['label']} — mean CV={np.mean(cv):.2f}\")\n",
    "    ax.set_xlabel('Zeitschritt')\n",
    "    ax.set_ylabel('Variationskoeffizient (std/mean)')\n",
    "    ax.set_xlim(-0.5, N_TIMESTEPS - 0.5)\n",
    "\n",
    "plt.suptitle('Variabilität pro Zeitschritt über Seeds (Coefficient of Variation)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modellvergleich\n",
    "\n",
    "Direkter Vergleich der gemittelten Attention-Profile aller vier Mechanismen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: Overlay of mean attention profiles\n",
    "ax = axes[0]\n",
    "for mk in model_keys:\n",
    "    weights = []\n",
    "    for seed in SEEDS:\n",
    "        w = all_data[mk][seed]['final']\n",
    "        if w is not None:\n",
    "            weights.append(w / w.sum())\n",
    "    if weights:\n",
    "        mean_w = np.mean(weights, axis=0)\n",
    "        std_w = np.std(weights, axis=0)\n",
    "        ax.plot(range(N_TIMESTEPS), mean_w, color=MODELS[mk]['color'],\n",
    "                linewidth=2, label=MODELS[mk]['label'])\n",
    "        ax.fill_between(range(N_TIMESTEPS), mean_w - std_w, mean_w + std_w,\n",
    "                        alpha=0.1, color=MODELS[mk]['color'])\n",
    "\n",
    "ax.axhline(y=1/N_TIMESTEPS, color='gray', linestyle='--', linewidth=1, label='Uniform')\n",
    "ax.set_xlabel('Zeitschritt (0=ältester, 49=neuester)')\n",
    "ax.set_ylabel('Attention Weight')\n",
    "ax.set_title('Mittleres Attention-Profil (über 5 Seeds)')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 49)\n",
    "\n",
    "# Right: Summary metrics\n",
    "ax = axes[1]\n",
    "metrics_data = []\n",
    "metric_labels = ['Norm. Entropie', 'Gew. in letzten 10', 'Cross-Seed CosSim']\n",
    "\n",
    "for mk in model_keys:\n",
    "    ents = entropies_per_model[mk]\n",
    "    \n",
    "    last10_vals = []\n",
    "    for seed in SEEDS:\n",
    "        w = all_data[mk][seed]['final']\n",
    "        if w is not None:\n",
    "            w_norm = w / w.sum()\n",
    "            last10_vals.append(w_norm[-10:].sum())\n",
    "    \n",
    "    cos_mean = np.mean(cosine_sims[mk]) if mk in cosine_sims else 0\n",
    "    \n",
    "    metrics_data.append([np.mean(ents), np.mean(last10_vals), cos_mean])\n",
    "\n",
    "metrics_data = np.array(metrics_data)\n",
    "x = np.arange(len(metric_labels))\n",
    "width = 0.18\n",
    "\n",
    "for i, mk in enumerate(model_keys):\n",
    "    ax.bar(x + i * width, metrics_data[i], width, color=MODELS[mk]['color'],\n",
    "           alpha=0.8, label=MODELS[mk]['label'], edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(metric_labels)\n",
    "ax.set_ylabel('Wert')\n",
    "ax.set_title('Zusammenfassung der Attention-Eigenschaften')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KL divergence from uniform distribution\n",
    "print('KL-Divergenz von Uniform-Verteilung (höher = fokussierter):')\n",
    "print(f\"{'Modell':30s} {'KL(attn||uniform)':>18s} {'Std':>8s}\")\n",
    "print('-' * 60)\n",
    "\n",
    "uniform = np.ones(N_TIMESTEPS) / N_TIMESTEPS\n",
    "\n",
    "for mk in model_keys:\n",
    "    kl_vals = []\n",
    "    for seed in SEEDS:\n",
    "        w = all_data[mk][seed]['final']\n",
    "        if w is not None:\n",
    "            w_norm = w / w.sum()\n",
    "            w_norm = np.clip(w_norm, 1e-12, None)\n",
    "            kl = scipy_entropy(w_norm, uniform)\n",
    "            kl_vals.append(kl)\n",
    "    print(f\"{MODELS[mk]['label']:30s} {np.mean(kl_vals):18.6f} {np.std(kl_vals):8.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Zusammenfassung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('ZUSAMMENFASSUNG: Warum Attention nicht hilft')\n",
    "print('=' * 80)\n",
    "\n",
    "for mk in model_keys:\n",
    "    ent_mean = np.mean(entropies_per_model[mk])\n",
    "    cos_mean = np.mean(cosine_sims[mk]) if mk in cosine_sims else float('nan')\n",
    "    \n",
    "    last10_vals = []\n",
    "    peak_positions = []\n",
    "    for seed in SEEDS:\n",
    "        w = all_data[mk][seed]['final']\n",
    "        if w is not None:\n",
    "            w_norm = w / w.sum()\n",
    "            last10_vals.append(w_norm[-10:].sum())\n",
    "            peak_positions.append(np.argmax(w_norm))\n",
    "    \n",
    "    print(f\"\\n--- {MODELS[mk]['label']} ---\")\n",
    "    print(f\"  Entropie:         {ent_mean:.4f} (1.0 = uniform)\")\n",
    "    print(f\"  Cross-Seed Sim:   {cos_mean:.4f}\")\n",
    "    print(f\"  Gewicht Last 10:  {np.mean(last10_vals)*100:.1f}% (erwartbar uniform: 20%)\")\n",
    "    print(f\"  Peak-Positionen:  {peak_positions}\")\n",
    "    \n",
    "    # Diagnosis\n",
    "    if ent_mean > 0.95:\n",
    "        print(f\"  >> DIAGNOSE: Nahezu uniform — Attention lernt keine Differenzierung\")\n",
    "    elif ent_mean > 0.85:\n",
    "        print(f\"  >> DIAGNOSE: Schwach fokussiert — minimaler Informationsgewinn\")\n",
    "    else:\n",
    "        print(f\"  >> DIAGNOSE: Fokussiert — Attention lernt ein Pattern\")\n",
    "    \n",
    "    if cos_mean < 0.9:\n",
    "        print(f\"  >> DIAGNOSE: Instabil über Seeds — kein robustes Signal\")\n",
    "    else:\n",
    "        print(f\"  >> DIAGNOSE: Stabil über Seeds\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print('FAZIT:')\n",
    "print('Die Attention-Mechanismen lernen keine informative Gewichtung der Zeitschritte.')\n",
    "print('Mögliche Erklärungen:')\n",
    "print('  1. Das LSTM kodiert die relevante temporale Information bereits im Hidden State')\n",
    "print('     -> Attention über Hidden States ist redundant')\n",
    "print('  2. Die Aufgabe (Steering Torque) hängt primär vom aktuellen Zustand ab,')\n",
    "print('     nicht von komplexen zeitlichen Mustern -> kein Attention-Vorteil')\n",
    "print('  3. Bei 50 Zeitschritten @ 10Hz (5s Window) gibt es wenig langreichweitige')\n",
    "print('     Abhängigkeiten, die Attention besser als LSTM erfassen könnte')\n",
    "print(f\"{'=' * 80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}