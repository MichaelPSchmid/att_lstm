{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Vollständige Evaluation — EPS Torque Prediction\n\nAlle Werte werden **aus den trainierten Checkpoints** berechnet. Keine vorberechneten `eval.json` als Input.\n\n| Schritt | Inhalt |\n|---------|--------|\n| 1 | Daten laden (einmalig) |\n| 2 | Checkpoint Discovery (8 Modelle × 5 Seeds) |\n| 3 | Inference — Predictions & Attention Weights |\n| 4 | Metriken (Sample-Level & Sequence-Level) |\n| 4b | Statistische Tests (Bootstrap, Permutationstests, Effektstärken) |\n| 5 | FLOPs & Parameter |\n| 6 | Inference Time Messung |\n| 7 | Ergebnis-Tabellen |\n| 8 | Figures (Attention, Tradeoff, Timeseries) |"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\MSchm\\Documents\\att_project\n",
      "PyTorch:      2.6.0+cu124\n",
      "CUDA:         True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gc\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "import winsound\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Project root (notebooks/ -> project root)\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if not (PROJECT_ROOT / 'config').exists():\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from config.loader import load_config, get_model_class\n",
    "from config.settings import get_preprocessed_paths\n",
    "from model.data_module import TimeSeriesDataModule\n",
    "from scripts.shared import (\n",
    "    MODELS, MODEL_BY_ID,\n",
    "    find_all_seed_checkpoints,\n",
    "    calculate_metrics_dict,\n",
    "    aggregate_metrics_per_sequence,\n",
    ")\n",
    "from scripts.compute_sequence_r2 import compute_per_sequence_r2\n",
    "from scripts.evaluate_model import (\n",
    "    measure_inference_time, calculate_flops, has_attention_support,\n",
    ")\n",
    "\n",
    "# Constants\n",
    "SEEDS = [7, 42, 94, 123, 231]\n",
    "VARIANT = 'no_dropout'\n",
    "ACCURACY_THRESHOLD = 0.05\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "print(f'Project root: {PROJECT_ROOT}')\n",
    "print(f'PyTorch:      {torch.__version__}')\n",
    "print(f'CUDA:         {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Daten laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading numpy file: C:\\Users\\MSchm\\Documents\\att_project\\data\\prepared_dataset\\HYUNDAI_SONATA_2020\\50_1_1_sF\\features_50_1_1_sF.npy\n",
      "Loading numpy file: C:\\Users\\MSchm\\Documents\\att_project\\data\\prepared_dataset\\HYUNDAI_SONATA_2020\\50_1_1_sF\\targets_50_1_1_sF.npy\n",
      "Loaded features: torch.Size([2201265, 50, 5])\n",
      "Loaded targets: torch.Size([2201265, 1])\n",
      "Loaded sequence_ids: 2201265 (4988 unique sequences)\n",
      "Sequence-level split (seed=0): 3491 train / 997 val / 500 test sequences\n",
      "Sample counts: 1539545 train / 440533 val / 221187 test\n",
      "\n",
      "Test samples:   221,187\n",
      "Test sequences: 500\n"
     ]
    }
   ],
   "source": [
    "config = load_config(str(PROJECT_ROOT / MODELS[0].config_no_dropout))\n",
    "data_config = config['data']\n",
    "\n",
    "paths = get_preprocessed_paths(\n",
    "    vehicle=data_config['vehicle'],\n",
    "    window_size=data_config['window_size'],\n",
    "    predict_size=data_config['predict_size'],\n",
    "    step_size=data_config['step_size'],\n",
    "    suffix='sF',\n",
    "    variant=data_config['variant'],\n",
    ")\n",
    "\n",
    "data_module = TimeSeriesDataModule(\n",
    "    feature_path=str(paths['features']),\n",
    "    target_path=str(paths['targets']),\n",
    "    sequence_ids_path=str(paths['sequence_ids']),\n",
    "    batch_size=256,\n",
    "    split_seed=data_config.get('split_seed', 0),\n",
    ")\n",
    "data_module.setup()\n",
    "\n",
    "# DataLoader with num_workers=0 (Windows/notebook compatibility)\n",
    "test_loader = DataLoader(\n",
    "    data_module.test_dataset, batch_size=256, shuffle=False, num_workers=0,\n",
    ")\n",
    "test_sequence_ids = data_module.get_split_sequence_ids('test')\n",
    "\n",
    "print(f'\\nTest samples:   {len(data_module.test_dataset):,}')\n",
    "print(f'Test sequences: {len(np.unique(test_sequence_ids)):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Checkpoint Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                                Seeds\n",
      "--------------------------------------------------\n",
      "  M1 MLP Last                       5/5\n",
      "  M2 MLP Flat                       5/5\n",
      "  M3 Small Baseline                 5/5\n",
      "  M4 Small + Simple Attn            5/5\n",
      "  M5 Medium Baseline                5/5\n",
      "  M6 Medium + Simple Attn           5/5\n",
      "  M7 Medium + Additive Attn         5/5\n",
      "  M8 Medium + Scaled DP             5/5\n",
      "\n",
      "Total: 40/40 checkpoints\n"
     ]
    }
   ],
   "source": [
    "all_checkpoints = {}\n",
    "\n",
    "print(f'{\"Model\":<35s} {\"Seeds\":>6s}')\n",
    "print('-' * 50)\n",
    "\n",
    "for mc in MODELS:\n",
    "    seed_ckpts = find_all_seed_checkpoints(mc, VARIANT)\n",
    "    all_checkpoints[mc.id] = seed_ckpts\n",
    "    found = len(seed_ckpts)\n",
    "    missing = [s for s in SEEDS if s not in seed_ckpts]\n",
    "    status = f'{found}/5'\n",
    "    if missing:\n",
    "        status += f'  (missing: {missing})'\n",
    "    print(f'  {mc.name:<33s} {status}')\n",
    "\n",
    "total = sum(len(v) for v in all_checkpoints.values())\n",
    "print(f'\\nTotal: {total}/40 checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference — Predictions & Attention Weights\n",
    "\n",
    "Lädt jeden Checkpoint, berechnet Predictions auf dem Test-Set.\n",
    "Für Attention-Modelle (M4, M6, M7, M8) werden die Attention Weights\n",
    "gleichzeitig extrahiert und über alle Test-Samples gemittelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, dataloader, device='cpu', extract_attention=False):\n",
    "    \"\"\"Run inference, optionally extracting attention weights.\n",
    "\n",
    "    Attention weights are averaged over samples incrementally to avoid\n",
    "    storing the full (N, seq_len, seq_len) matrix for additive attention.\n",
    "\n",
    "    Returns:\n",
    "        predictions, targets                       (if extract_attention=False)\n",
    "        predictions, targets, avg_attention_1d     (if extract_attention=True)\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds, all_targets = [], []\n",
    "    attn_sum = None\n",
    "    n_attn_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "\n",
    "            if extract_attention:\n",
    "                outputs, attn = model(X_batch, return_attention=True)\n",
    "                attn_np = attn.cpu().numpy()\n",
    "\n",
    "                if attn_np.ndim == 2:\n",
    "                    # Simple / Scaled DP: (batch, seq_len)\n",
    "                    batch_sum = attn_np.sum(axis=0)\n",
    "                elif attn_np.ndim == 3:\n",
    "                    # Additive (M7): (batch, query, key)\n",
    "                    # Average over query dim -> importance per key position\n",
    "                    batch_sum = attn_np.mean(axis=1).sum(axis=0)\n",
    "                else:\n",
    "                    batch_sum = np.zeros(50)\n",
    "\n",
    "                if attn_sum is None:\n",
    "                    attn_sum = batch_sum\n",
    "                else:\n",
    "                    attn_sum += batch_sum\n",
    "                n_attn_samples += len(attn_np)\n",
    "            else:\n",
    "                outputs = model(X_batch)\n",
    "\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_targets.append(Y_batch.numpy())\n",
    "\n",
    "    predictions = np.concatenate(all_preds, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    if extract_attention and attn_sum is not None:\n",
    "        avg_attention = attn_sum / n_attn_samples\n",
    "        return predictions, targets, avg_attention\n",
    "    return predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "M1 MLP Last:\n",
      "  Seed 7: val_loss=0.003477, samples=221,187\n",
      "  Seed 42: val_loss=0.003500, samples=221,187\n",
      "  Seed 94: val_loss=0.003454, samples=221,187\n",
      "  Seed 123: val_loss=0.003421, samples=221,187\n",
      "  Seed 231: val_loss=0.003470, samples=221,187\n",
      "\n",
      "M2 MLP Flat:\n",
      "  Seed 7: val_loss=0.002648, samples=221,187\n",
      "  Seed 42: val_loss=0.002598, samples=221,187\n",
      "  Seed 94: val_loss=0.002588, samples=221,187\n",
      "  Seed 123: val_loss=0.002775, samples=221,187\n",
      "  Seed 231: val_loss=0.002592, samples=221,187\n",
      "\n",
      "M3 Small Baseline:\n",
      "  Seed 7: val_loss=0.001940, samples=221,187\n",
      "  Seed 42: val_loss=0.001973, samples=221,187\n",
      "  Seed 94: val_loss=0.001993, samples=221,187\n",
      "  Seed 123: val_loss=0.001958, samples=221,187\n",
      "  Seed 231: val_loss=0.001933, samples=221,187\n",
      "\n",
      "M4 Small + Simple Attn:\n",
      "  Seed 7: val_loss=0.001993, samples=221,187\n",
      "  Seed 42: val_loss=0.001946, samples=221,187\n",
      "  Seed 94: val_loss=0.001987, samples=221,187\n",
      "  Seed 123: val_loss=0.001939, samples=221,187\n",
      "  Seed 231: val_loss=0.001964, samples=221,187\n",
      "\n",
      "M5 Medium Baseline:\n",
      "  Seed 7: val_loss=0.001952, samples=221,187\n",
      "  Seed 42: val_loss=0.001975, samples=221,187\n",
      "  Seed 94: val_loss=0.001965, samples=221,187\n",
      "  Seed 123: val_loss=0.001928, samples=221,187\n",
      "  Seed 231: val_loss=0.002002, samples=221,187\n",
      "\n",
      "M6 Medium + Simple Attn:\n",
      "  Seed 7: val_loss=0.001904, samples=221,187\n",
      "  Seed 42: val_loss=0.001909, samples=221,187\n",
      "  Seed 94: val_loss=0.001925, samples=221,187\n",
      "  Seed 123: val_loss=0.001928, samples=221,187\n",
      "  Seed 231: val_loss=0.001921, samples=221,187\n",
      "\n",
      "M7 Medium + Additive Attn:\n",
      "  Seed 7: val_loss=0.001907, samples=221,187\n",
      "  Seed 42: val_loss=0.001902, samples=221,187\n",
      "  Seed 94: val_loss=0.001929, samples=221,187\n",
      "  Seed 123: val_loss=0.001915, samples=221,187\n",
      "  Seed 231: val_loss=0.001943, samples=221,187\n",
      "\n",
      "M8 Medium + Scaled DP:\n",
      "  Seed 7: val_loss=0.001985, samples=221,187\n",
      "  Seed 42: val_loss=0.001998, samples=221,187\n",
      "  Seed 94: val_loss=0.001977, samples=221,187\n",
      "  Seed 123: val_loss=0.001994, samples=221,187\n",
      "  Seed 231: val_loss=0.001996, samples=221,187\n",
      "\n",
      "Done. 40 model-seed evaluations.\n"
     ]
    }
   ],
   "source": [
    "results = {}  # {model_id: {seed: {predictions, targets, attention}}}\n",
    "\n",
    "for mc in MODELS:\n",
    "    config_path = PROJECT_ROOT / mc.config_no_dropout\n",
    "    cfg = load_config(str(config_path))\n",
    "    model_class = get_model_class(cfg['model']['type'])\n",
    "\n",
    "    results[mc.id] = {}\n",
    "    seed_ckpts = all_checkpoints[mc.id]\n",
    "\n",
    "    print(f'\\n{mc.name}:')\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        if seed not in seed_ckpts:\n",
    "            print(f'  Seed {seed}: MISSING')\n",
    "            continue\n",
    "\n",
    "        ckpt_path, val_loss = seed_ckpts[seed]\n",
    "        model = model_class.load_from_checkpoint(str(ckpt_path), map_location='cpu')\n",
    "\n",
    "        has_attn = has_attention_support(model)\n",
    "\n",
    "        if has_attn:\n",
    "            preds, targs, attn = run_inference(\n",
    "                model, test_loader, DEVICE, extract_attention=True,\n",
    "            )\n",
    "        else:\n",
    "            preds, targs = run_inference(model, test_loader, DEVICE)\n",
    "            attn = None\n",
    "\n",
    "        results[mc.id][seed] = {\n",
    "            'predictions': preds,\n",
    "            'targets': targs,\n",
    "            'attention': attn,\n",
    "        }\n",
    "\n",
    "        print(f'  Seed {seed}: val_loss={val_loss:.6f}, samples={len(preds):,}')\n",
    "\n",
    "        del model\n",
    "        gc.collect()\n",
    "\n",
    "n_evals = sum(len(v) for v in results.values())\n",
    "print(f'\\nDone. {n_evals} model-seed evaluations.')\n",
    "winsound.PlaySound(\"SystemHand\", winsound.SND_ALIAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metriken berechnen\n",
    "\n",
    "- **Sample-Level:** MSE, RMSE, MAE, R², Accuracy\n",
    "- **Sequence-Level:** RMSE, MAE, Accuracy, R² (pro Sequenz, dann gemittelt)\n",
    "- Jeweils mean ± std über 5 Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics computed.\n"
     ]
    }
   ],
   "source": [
    "metrics_rows = []\n",
    "\n",
    "for mc in MODELS:\n",
    "    seed_data = {\n",
    "        'mse': [], 'rmse': [], 'mae': [], 'r2': [], 'accuracy': [],\n",
    "        'seq_rmse': [], 'seq_mae': [], 'seq_accuracy': [], 'seq_r2': [],\n",
    "    }\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        if seed not in results[mc.id]:\n",
    "            continue\n",
    "\n",
    "        preds = results[mc.id][seed]['predictions']\n",
    "        targs = results[mc.id][seed]['targets']\n",
    "\n",
    "        # Sample-level metrics\n",
    "        m = calculate_metrics_dict(preds, targs, ACCURACY_THRESHOLD)\n",
    "        seed_data['mse'].append(m['mse'])\n",
    "        seed_data['rmse'].append(m['rmse'])\n",
    "        seed_data['mae'].append(m['mae'])\n",
    "        seed_data['r2'].append(m['r2'])\n",
    "        seed_data['accuracy'].append(m['accuracy'])\n",
    "\n",
    "        # Sequence-level metrics\n",
    "        _, seq_summary = aggregate_metrics_per_sequence(\n",
    "            preds, targs, test_sequence_ids, ACCURACY_THRESHOLD,\n",
    "        )\n",
    "        seed_data['seq_rmse'].append(seq_summary['rmse_mean'])\n",
    "        seed_data['seq_mae'].append(seq_summary['mae_mean'])\n",
    "        seed_data['seq_accuracy'].append(seq_summary['accuracy_mean'])\n",
    "\n",
    "        # Sequence-level R-squared\n",
    "        mean_r2, _ = compute_per_sequence_r2(preds, targs, test_sequence_ids)\n",
    "        seed_data['seq_r2'].append(mean_r2)\n",
    "\n",
    "    row = {'model': mc.id.upper(), 'name': mc.name}\n",
    "    for key, vals in seed_data.items():\n",
    "        if vals:\n",
    "            row[f'{key}_mean'] = np.mean(vals)\n",
    "            row[f'{key}_std'] = np.std(vals)\n",
    "    metrics_rows.append(row)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows)\n",
    "print('Metrics computed.')\n",
    "metrics_df[['model', 'name', 'accuracy_mean', 'accuracy_std',\n",
    "            'rmse_mean', 'rmse_std', 'seq_r2_mean', 'seq_r2_std']].round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8w258h2twsb",
   "source": "## 4b. Statistische Tests (Sequenz-Ebene)\n\nBlock-Bootstrap CIs, Permutationstests und Effektstärken. Funktionen aus `scripts/sequence_level_evaluation.py` werden per Import wiederverwendet. Alle Berechnungen nutzen die vorhandenen Predictions im `results`-Dict — keine erneute Inference.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5oeo1aa6vdr",
   "source": "from scripts.sequence_level_evaluation import (\n    bootstrap_ci_sequences,\n    cohens_d_paired_sequences,\n    permutation_test_sequences,\n    multi_seed_sequence_analysis,\n    run_all_comparisons,\n    _compute_seq_metric_arrays,\n    COMPARISON_PAIRS,\n    METRICS,\n    _significance_stars,\n    _effect_size_category,\n)\n\n# Compute per-sequence metric arrays for all models x seeds\nall_seq_metrics = {}  # {model_id: {seed: {metric: array}}}\n\nfor mc in MODELS:\n    all_seq_metrics[mc.id] = {}\n    for seed in SEEDS:\n        if seed not in results[mc.id]:\n            continue\n        preds = results[mc.id][seed]['predictions']\n        targs = results[mc.id][seed]['targets']\n        seq_arrays = _compute_seq_metric_arrays(\n            preds, targs, test_sequence_ids, ACCURACY_THRESHOLD,\n        )\n        all_seq_metrics[mc.id][seed] = seq_arrays\n\nn_models = len(all_seq_metrics)\nn_total = sum(len(v) for v in all_seq_metrics.values())\nprint(f'Per-sequence metrics computed: {n_models} models, {n_total} model-seed combinations')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "jlct02e7y5a",
   "source": "# Block-Bootstrap CIs + Law of Total Variance for all models\nbootstrap_results = {}  # {model_id: aggregated multi-seed result}\nseed_stability = {}  # {model_id: per-seed accuracy values}\n\nfor mc in MODELS:\n    model_id = mc.id\n    seed_data = all_seq_metrics[model_id]\n    seeds_available = sorted(seed_data.keys())\n\n    # Per-seed bootstrap CIs\n    seed_bootstrap_ci = {}\n    seed_point_metrics = {}\n\n    for seed in seeds_available:\n        seq_arrays = seed_data[seed]\n        point = {m: float(np.mean(seq_arrays[m])) for m in METRICS}\n        seed_point_metrics[seed] = point\n\n        ci = {}\n        for metric in METRICS:\n            ci[metric] = bootstrap_ci_sequences(\n                seq_arrays[metric], n_bootstrap=1000, seed=42,\n            )\n        seed_bootstrap_ci[seed] = ci\n\n    # Multi-seed aggregation (law of total variance)\n    aggregated = multi_seed_sequence_analysis(seed_bootstrap_ci, seed_point_metrics)\n    bootstrap_results[model_id] = aggregated\n    seed_stability[model_id] = {\n        'per_seed_accuracy': [seed_point_metrics[s]['accuracy'] for s in seeds_available],\n        'sigma_seed': aggregated['accuracy']['std_seed'],\n    }\n\n# --- Combined uncertainty table ---\nrows = []\nfor mc in MODELS:\n    r = bootstrap_results[mc.id]\n    rows.append({\n        'Model': mc.id.upper(),\n        'Name': mc.name,\n        'Accuracy (%)': f\"{r['accuracy']['mean']:.2f} \\u00b1 {r['accuracy']['std']:.2f}\",\n        'RMSE': f\"{r['rmse']['mean']:.4f} \\u00b1 {r['rmse']['std']:.4f}\",\n        'MAE': f\"{r['mae']['mean']:.4f} \\u00b1 {r['mae']['std']:.4f}\",\n    })\nprint('Bootstrap CIs (combined uncertainty, 1000 samples \\u00d7 5 seeds):')\ndisplay(pd.DataFrame(rows))\n\n# --- Seed stability ---\nprint('\\nSeed Stability (per-seed accuracy):')\nstab_rows = []\nfor mc in MODELS:\n    s = seed_stability[mc.id]\n    acc_vals = s['per_seed_accuracy']\n    row = {'Model': mc.id.upper()}\n    for seed, val in zip(SEEDS, acc_vals):\n        row[f'Seed {seed}'] = f'{val:.2f}'\n    row['\\u03c3_seed'] = f\"{s['sigma_seed']:.2f}\"\n    stab_rows.append(row)\ndisplay(pd.DataFrame(stab_rows))\n\n# --- Uncertainty decomposition ---\nprint('\\nUncertainty Decomposition (Law of Total Variance):')\ndecomp_rows = []\nfor mc in MODELS:\n    r = bootstrap_results[mc.id]\n    acc = r['accuracy']\n    sigma_total = acc['std']\n    sigma_boot = acc['std_bootstrap']\n    sigma_seed = acc['std_seed']\n    seed_frac = (sigma_seed**2 / sigma_total**2 * 100) if sigma_total > 0 else 0\n    decomp_rows.append({\n        'Model': mc.id.upper(),\n        '\\u03c3_total': f'{sigma_total:.2f}',\n        '\\u03c3_bootstrap': f'{sigma_boot:.2f}',\n        '\\u03c3_seed': f'{sigma_seed:.2f}',\n        'Seed Variance %': f'{seed_frac:.0f}%',\n    })\ndisplay(pd.DataFrame(decomp_rows))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "leqnjczf1xe",
   "source": "# Seed-averaged per-sequence metrics for pairwise comparisons\navg_seq_metrics = {}  # {MODEL_ID: {metric: array}}\n\nfor mc in MODELS:\n    model_id = mc.id.upper()\n    seed_data = all_seq_metrics[mc.id]\n    seeds_available = sorted(seed_data.keys())\n\n    avg = {}\n    for metric in METRICS:\n        avg[metric] = np.mean(\n            [seed_data[s][metric] for s in seeds_available], axis=0,\n        )\n    avg_seq_metrics[model_id] = avg\n\n# Run all pairwise comparisons (10,000 sign-flip permutations)\ncomparison_results = run_all_comparisons(\n    COMPARISON_PAIRS, avg_seq_metrics,\n    n_permutations=10000, seed=42,\n)\n\n# Display results\ncomp_rows = []\nfor c in comparison_results:\n    acc = c['accuracy']\n    rmse = c['rmse']\n    mae = c['mae']\n    acc_stars = _significance_stars(acc['p_value'])\n    rmse_stars = _significance_stars(rmse['p_value'])\n    mae_stars = _significance_stars(mae['p_value'])\n\n    comp_rows.append({\n        'Comparison': f\"{c['model_a']} \\u2192 {c['model_b']}\",\n        'Category': c['category'],\n        '\\u0394 Acc (%)': f\"{acc['observed_diff']:+.2f}{acc_stars}\",\n        '\\u0394 RMSE': f\"{rmse['observed_diff']:+.4f}{rmse_stars}\",\n        '\\u0394 MAE': f\"{mae['observed_diff']:+.4f}{mae_stars}\",\n        'd(Acc)': f\"{acc['cohens_d']:+.3f}\",\n        'd(RMSE)': f\"{rmse['cohens_d']:+.3f}\",\n        'd(MAE)': f\"{mae['cohens_d']:+.3f}\",\n    })\n\nprint(f'Pairwise Comparisons (10,000 permutations, seed=42):')\nprint('Significance: * p<0.05, ** p<0.01, *** p<0.001')\nprint(\"Cohen's d: |d|<0.2 negligible, 0.2-0.5 small, 0.5-0.8 medium, >0.8 large\")\nprint(\"d sign: positive = B better (higher accuracy, lower RMSE/MAE)\\n\")\npd.DataFrame(comp_rows)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. FLOPs & Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = torch.randn(1, data_config['window_size'], 5)\n",
    "flops_rows = []\n",
    "\n",
    "for mc in MODELS:\n",
    "    cfg = load_config(str(PROJECT_ROOT / mc.config_no_dropout))\n",
    "    model_class = get_model_class(cfg['model']['type'])\n",
    "\n",
    "    seed_ckpts = all_checkpoints[mc.id]\n",
    "    any_seed = next(iter(seed_ckpts))\n",
    "    ckpt_path, _ = seed_ckpts[any_seed]\n",
    "    model = model_class.load_from_checkpoint(str(ckpt_path), map_location='cpu')\n",
    "\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    flops_result = calculate_flops(model, sample_input.clone())\n",
    "\n",
    "    flops_rows.append({\n",
    "        'model': mc.id.upper(),\n",
    "        'params': n_params,\n",
    "        'flops': flops_result['flops'],\n",
    "        'flops_fmt': flops_result['flops_formatted'],\n",
    "        'macs_fmt': flops_result['macs_formatted'],\n",
    "    })\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "flops_df = pd.DataFrame(flops_rows)\n",
    "flops_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference Time Messung\n",
    "\n",
    "> **Hinweis:** Diese Zelle isoliert ausführen. Keine anderen rechenintensiven Prozesse parallel laufen lassen.\n",
    ">\n",
    "> Messung: CPU, single-thread (`torch.set_num_threads(1)`), 100 Warmup, 5 Runs × 1000 Samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m ckpt_path, _ = seed_ckpts[\u001b[32m42\u001b[39m]\n\u001b[32m      9\u001b[39m model = model_class.load_from_checkpoint(\u001b[38;5;28mstr\u001b[39m(ckpt_path), map_location=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m timing = measure_inference_time(\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     model, \u001b[43msample_input\u001b[49m.clone(),\n\u001b[32m     13\u001b[39m     warmup_iterations=\u001b[32m100\u001b[39m,\n\u001b[32m     14\u001b[39m     num_samples=\u001b[32m1000\u001b[39m,\n\u001b[32m     15\u001b[39m     num_runs=\u001b[32m5\u001b[39m,\n\u001b[32m     16\u001b[39m     device=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m inference_rows.append({\n\u001b[32m     20\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m: mc.id.upper(),\n\u001b[32m     21\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmean_ms\u001b[39m\u001b[33m'\u001b[39m: timing[\u001b[33m'\u001b[39m\u001b[33mmean_ms\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mp99_std_ms\u001b[39m\u001b[33m'\u001b[39m: timing[\u001b[33m'\u001b[39m\u001b[33mp99_std_ms\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     29\u001b[39m })\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmc.name\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<35s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  P95=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtiming[\u001b[33m\"\u001b[39m\u001b[33mp95_ms\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m +/- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtiming[\u001b[33m\"\u001b[39m\u001b[33mp95_std_ms\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ms\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'sample_input' is not defined"
     ]
    }
   ],
   "source": [
    "inference_rows = []\n",
    "\n",
    "for mc in MODELS:\n",
    "    cfg = load_config(str(PROJECT_ROOT / mc.config_no_dropout))\n",
    "    model_class = get_model_class(cfg['model']['type'])\n",
    "\n",
    "    seed_ckpts = all_checkpoints[mc.id]\n",
    "    ckpt_path, _ = seed_ckpts[42]\n",
    "    model = model_class.load_from_checkpoint(str(ckpt_path), map_location='cpu')\n",
    "\n",
    "    timing = measure_inference_time(\n",
    "        model, sample_input.clone(),\n",
    "        warmup_iterations=100,\n",
    "        num_samples=1000,\n",
    "        num_runs=5,\n",
    "        device='cpu',\n",
    "    )\n",
    "\n",
    "    inference_rows.append({\n",
    "        'model': mc.id.upper(),\n",
    "        'mean_ms': timing['mean_ms'],\n",
    "        'std_ms': timing['std_ms'],\n",
    "        'p50_ms': timing['p50_ms'],\n",
    "        'p50_std_ms': timing['p50_std_ms'],\n",
    "        'p95_ms': timing['p95_ms'],\n",
    "        'p95_std_ms': timing['p95_std_ms'],\n",
    "        'p99_ms': timing['p99_ms'],\n",
    "        'p99_std_ms': timing['p99_std_ms'],\n",
    "    })\n",
    "\n",
    "    print(f'  {mc.name:<35s}  P95={timing[\"p95_ms\"]:.3f} +/- {timing[\"p95_std_ms\"]:.3f} ms')\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "inference_df = pd.DataFrame(inference_rows)\n",
    "inference_df\n",
    "\n",
    "winsound.PlaySound(\"SystemHand\", winsound.SND_ALIAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ergebnis-Tabellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt(row, key, decimals=4):\n",
    "    \"\"\"Format mean +/- std.\"\"\"\n",
    "    m = row.get(f'{key}_mean', float('nan'))\n",
    "    s = row.get(f'{key}_std', float('nan'))\n",
    "    return f'{m:.{decimals}f} +/- {s:.{decimals}f}'\n",
    "\n",
    "\n",
    "main = metrics_df[['model', 'name']].copy()\n",
    "main = main.merge(flops_df[['model', 'params']], on='model')\n",
    "main = main.merge(inference_df[['model', 'p95_ms', 'p95_std_ms']], on='model')\n",
    "\n",
    "main['Accuracy (%)'] = metrics_df.apply(lambda r: fmt(r, 'accuracy', 2), axis=1)\n",
    "main['RMSE'] = metrics_df.apply(lambda r: fmt(r, 'rmse'), axis=1)\n",
    "main['MAE'] = metrics_df.apply(lambda r: fmt(r, 'mae'), axis=1)\n",
    "main['R2 (sample)'] = metrics_df.apply(lambda r: fmt(r, 'r2'), axis=1)\n",
    "main['R2 (sequence)'] = metrics_df.apply(lambda r: fmt(r, 'seq_r2'), axis=1)\n",
    "main['Seq Accuracy (%)'] = metrics_df.apply(lambda r: fmt(r, 'seq_accuracy', 2), axis=1)\n",
    "main['Seq RMSE'] = metrics_df.apply(lambda r: fmt(r, 'seq_rmse'), axis=1)\n",
    "main['P95 (ms)'] = main.apply(\n",
    "    lambda r: f\"{r['p95_ms']:.3f} +/- {r['p95_std_ms']:.3f}\", axis=1,\n",
    ")\n",
    "\n",
    "display_cols = [\n",
    "    'model', 'name', 'params',\n",
    "    'Accuracy (%)', 'RMSE', 'MAE',\n",
    "    'R2 (sample)', 'R2 (sequence)',\n",
    "    'Seq Accuracy (%)', 'Seq RMSE',\n",
    "    'P95 (ms)',\n",
    "]\n",
    "main[display_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import scienceplots  # noqa: F401\n",
    "    plt.style.use(['science', 'ieee'])\n",
    "except ImportError:\n",
    "    print('WARNING: scienceplots not installed, using default style')\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'text.usetex': True,\n",
    "    'pgf.texsystem': 'pdflatex',\n",
    "    'pgf.rcfonts': False,\n",
    "    'pgf.preamble': '\\n'.join([\n",
    "        r'\\usepackage[utf8]{inputenc}',\n",
    "        r'\\usepackage[T1]{fontenc}',\n",
    "        r'\\usepackage{amsmath}',\n",
    "        r'\\usepackage{siunitx}',\n",
    "        r'\\providecommand{\\mathdefault}[1]{#1}',\n",
    "    ]),\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Computer Modern Roman'],\n",
    "    'figure.figsize': (3.5, 2.5),\n",
    "    'savefig.bbox': 'tight',\n",
    "    'savefig.pad_inches': 0.02,\n",
    "    'lines.linewidth': 1.0,\n",
    "    'lines.markersize': 4,\n",
    "    'axes.grid': False,\n",
    "    'legend.framealpha': 0.95,\n",
    "    'legend.edgecolor': 'none',\n",
    "    'savefig.dpi': 300,\n",
    "})\n",
    "\n",
    "MATLAB_BLUE = (0/255, 114/255, 189/255)\n",
    "MATLAB_ORANGE = (217/255, 83/255, 25/255)\n",
    "MATLAB_PURPLE = (126/255, 47/255, 142/255)\n",
    "\n",
    "FIGURES_DIR = PROJECT_ROOT / 'figures'\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_figure(fig, name):\n",
    "    \"\"\"Save figure as PGF, PDF, and PNG.\"\"\"\n",
    "    for ext in ['pgf', 'pdf', 'png']:\n",
    "        path = FIGURES_DIR / f'{name}.{ext}'\n",
    "        fig.savefig(path)\n",
    "        print(f'  Saved: {path.relative_to(PROJECT_ROOT)}')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "print('Figure styling configured.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8a. Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average attention profiles across seeds for each attention model\n",
    "attention_profiles = {}\n",
    "\n",
    "for model_id in ['m4', 'm6', 'm7', 'm8']:\n",
    "    seed_weights = []\n",
    "    for seed in SEEDS:\n",
    "        if seed not in results[model_id]:\n",
    "            continue\n",
    "        attn = results[model_id][seed]['attention']\n",
    "        if attn is None:\n",
    "            continue\n",
    "        seed_weights.append(attn)\n",
    "\n",
    "    if not seed_weights:\n",
    "        print(f'  {model_id.upper()}: no attention data')\n",
    "        continue\n",
    "\n",
    "    combined = np.mean(seed_weights, axis=0)\n",
    "    combined = combined / combined.sum()  # Normalize to sum=1\n",
    "    attention_profiles[model_id] = combined\n",
    "\n",
    "    last5 = combined[-5:].sum() * 100\n",
    "    last10 = combined[-10:].sum() * 100\n",
    "    last20 = combined[-20:].sum() * 100\n",
    "    peak = np.argmax(combined)\n",
    "    print(f'  {model_id.upper()}: Last5={last5:.1f}%, Last10={last10:.1f}%, '\n",
    "          f'Last20={last20:.1f}%, Peak={peak}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: 3 subplots (M6, M7, M8)\n",
    "models_to_plot = [\n",
    "    ('m6', 'Simple Attention', MATLAB_BLUE),\n",
    "    ('m7', 'Additive Attention', MATLAB_ORANGE),\n",
    "    ('m8', 'Scaled Dot-Product', MATLAB_PURPLE),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(7.16, 2.2), sharey=False)\n",
    "\n",
    "for ax, (mid, label, color) in zip(axes, models_to_plot):\n",
    "    if mid not in attention_profiles:\n",
    "        ax.text(0.5, 0.5, 'No data', transform=ax.transAxes, ha='center')\n",
    "        continue\n",
    "\n",
    "    w = attention_profiles[mid]\n",
    "    t = np.arange(len(w))\n",
    "\n",
    "    ax.plot(t, w, color=color, linewidth=1.2, linestyle='-')\n",
    "    ax.axhline(\n",
    "        y=1.0 / 50, color='gray', linestyle='--', linewidth=0.7,\n",
    "        label=r'Uniform ($\\frac{1}{50}$)',\n",
    "    )\n",
    "    ax.set_xlabel('Time Step')\n",
    "    ax.set_title(label, fontsize=8)\n",
    "    ax.set_xlim(0, 49)\n",
    "    ax.set_ylim(bottom=0)\n",
    "\n",
    "axes[0].set_ylabel('Attention Weight')\n",
    "axes[0].legend(fontsize=6, loc='upper left')\n",
    "fig.tight_layout()\n",
    "\n",
    "save_figure(fig, 'attention_weights_plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8b. Inference-Accuracy Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build data from computed metrics + inference times\n",
    "tradeoff_data = {}\n",
    "for _, mrow in metrics_df.iterrows():\n",
    "    mid = mrow['model']\n",
    "    irow = inference_df[inference_df['model'] == mid].iloc[0]\n",
    "\n",
    "    if mid in ('M1', 'M2'):\n",
    "        mtype = 'mlp'\n",
    "    elif mid in ('M3', 'M5'):\n",
    "        mtype = 'lstm'\n",
    "    else:\n",
    "        mtype = 'lstm_attn'\n",
    "\n",
    "    tradeoff_data[mid] = {\n",
    "        'accuracy': mrow['accuracy_mean'],\n",
    "        'p95_ms': irow['p95_ms'],\n",
    "        'type': mtype,\n",
    "    }\n",
    "\n",
    "# Marker and color maps\n",
    "MARKERS = {'mlp': 's', 'lstm': 'o', 'lstm_attn': '^'}\n",
    "TYPE_COLORS = {'mlp': '#7f7f7f', 'lstm': '#1f77b4', 'lstm_attn': '#ff7f0e'}\n",
    "MARKER_SIZES = {'mlp': 35, 'lstm': 45, 'lstm_attn': 50}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3.5, 2.8))\n",
    "\n",
    "plotted_types = set()\n",
    "for mid, d in tradeoff_data.items():\n",
    "    mtype = d['type']\n",
    "    label = None\n",
    "    if mtype not in plotted_types:\n",
    "        label = {'mlp': 'MLP Baseline', 'lstm': 'LSTM Baseline',\n",
    "                 'lstm_attn': 'LSTM + Attention'}[mtype]\n",
    "        plotted_types.add(mtype)\n",
    "\n",
    "    ax.scatter(\n",
    "        d['p95_ms'], d['accuracy'],\n",
    "        marker=MARKERS[mtype], s=MARKER_SIZES[mtype],\n",
    "        c=TYPE_COLORS[mtype], edgecolors='white', linewidths=0.5,\n",
    "        label=label, zorder=4,\n",
    "    )\n",
    "\n",
    "# Highlight M3 (Pareto-optimal)\n",
    "m3 = tradeoff_data['M3']\n",
    "ax.scatter(\n",
    "    m3['p95_ms'], m3['accuracy'],\n",
    "    marker='o', s=120, facecolors='none',\n",
    "    edgecolors='#d62728', linewidths=1.5, zorder=5,\n",
    ")\n",
    "\n",
    "# Model labels\n",
    "label_offsets = {\n",
    "    'M1': (5, -2), 'M2': (5, -2), 'M3': (5, 3), 'M4': (5, -5),\n",
    "    'M5': (-8, 3), 'M6': (-5, -5), 'M7': (5, 3), 'M8': (5, -5),\n",
    "}\n",
    "\n",
    "for mid, d in tradeoff_data.items():\n",
    "    x_off, y_off = label_offsets[mid]\n",
    "    ha = 'left' if x_off >= 0 else 'right'\n",
    "    va = 'bottom' if y_off > 0 else 'top'\n",
    "    text = r'\\textbf{M3}' if mid == 'M3' else mid\n",
    "\n",
    "    ax.annotate(\n",
    "        text, (d['p95_ms'], d['accuracy']),\n",
    "        xytext=(x_off, y_off), textcoords='offset points',\n",
    "        fontsize=7, ha=ha, va=va,\n",
    "    )\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(0.04, 7)\n",
    "ax.set_xticks([0.05, 0.1, 0.5, 1, 2, 5])\n",
    "ax.set_xticklabels(['0.05', '0.1', '0.5', '1', '2', '5'])\n",
    "ax.set_xlabel(r'Inference Time P95 (ms)')\n",
    "ax.set_ylabel(r'Accuracy (\\%)')\n",
    "ax.legend(loc='lower right', fontsize=7)\n",
    "\n",
    "save_figure(fig, 'inference_accuracy_tradeoff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8c. Prediction Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use seed 42, models M3/M5/M6\n",
    "PRED_MODELS = ['m3', 'm5', 'm6']\n",
    "PRED_SEED = 42\n",
    "\n",
    "# Compute per-sequence RMSE using M5 as reference\n",
    "ref_preds = results['m5'][PRED_SEED]['predictions'].flatten()\n",
    "ref_targs = results['m5'][PRED_SEED]['targets'].flatten()\n",
    "\n",
    "unique_seqs = np.unique(test_sequence_ids)\n",
    "seq_rmse = {}\n",
    "for sid in unique_seqs:\n",
    "    mask = test_sequence_ids == sid\n",
    "    p = ref_preds[mask]\n",
    "    t = ref_targs[mask]\n",
    "    seq_rmse[sid] = np.sqrt(np.mean((p - t) ** 2))\n",
    "\n",
    "rmse_values = np.array(list(seq_rmse.values()))\n",
    "seq_ids_arr = np.array(list(seq_rmse.keys()))\n",
    "\n",
    "\n",
    "def pick_sequence(lo_pct, hi_pct):\n",
    "    \"\"\"Pick sequence with RMSE closest to midpoint of percentile range.\"\"\"\n",
    "    lo = np.percentile(rmse_values, lo_pct)\n",
    "    hi = np.percentile(rmse_values, hi_pct)\n",
    "    mask = (rmse_values >= lo) & (rmse_values <= hi)\n",
    "    if not mask.any():\n",
    "        mid = np.percentile(rmse_values, (lo_pct + hi_pct) / 2)\n",
    "        idx = np.argmin(np.abs(rmse_values - mid))\n",
    "        return seq_ids_arr[idx]\n",
    "    mid = (lo + hi) / 2\n",
    "    candidates = rmse_values[mask]\n",
    "    cand_ids = seq_ids_arr[mask]\n",
    "    best = np.argmin(np.abs(candidates - mid))\n",
    "    return cand_ids[best]\n",
    "\n",
    "\n",
    "selected = {\n",
    "    'good': pick_sequence(10, 25),\n",
    "    'median': pick_sequence(45, 55),\n",
    "    'difficult': pick_sequence(75, 90),\n",
    "}\n",
    "\n",
    "for label, sid in selected.items():\n",
    "    print(f'  {label}: seq_id={sid}, RMSE={seq_rmse[sid]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    ('good', 'Good Prediction'),\n",
    "    ('median', 'Median Prediction'),\n",
    "    ('difficult', 'Difficult Prediction'),\n",
    "]\n",
    "\n",
    "model_colors = {'m3': MATLAB_BLUE, 'm5': MATLAB_ORANGE, 'm6': MATLAB_PURPLE}\n",
    "model_styles = {'m3': '--', 'm5': '-.', 'm6': ':'}\n",
    "model_labels = {'m3': 'M3 (Small)', 'm5': 'M5 (Medium)', 'm6': 'M6 (+ Attn)'}\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(3.5, 5.5), sharex=True)\n",
    "\n",
    "for ax, (cat, title_prefix) in zip(axes, categories):\n",
    "    sid = selected[cat]\n",
    "    mask = test_sequence_ids == sid\n",
    "\n",
    "    gt = results['m5'][PRED_SEED]['targets'].flatten()[mask]\n",
    "    timesteps = np.arange(len(gt))\n",
    "\n",
    "    ax.plot(\n",
    "        timesteps, gt, color='black', linewidth=1.0,\n",
    "        linestyle='-', label='Ground Truth',\n",
    "    )\n",
    "\n",
    "    for mid in PRED_MODELS:\n",
    "        p = results[mid][PRED_SEED]['predictions'].flatten()[mask]\n",
    "        ax.plot(\n",
    "            timesteps, p, color=model_colors[mid], linewidth=0.8,\n",
    "            linestyle=model_styles[mid], label=model_labels[mid],\n",
    "        )\n",
    "\n",
    "    rmse_m5 = seq_rmse[sid]\n",
    "    ax.set_title(\n",
    "        f'{title_prefix} (RMSE$_{{\\\\mathrm{{M5}}}}={rmse_m5:.3f}$)',\n",
    "        fontsize=8,\n",
    "    )\n",
    "    ax.set_ylabel('Torque (norm.)')\n",
    "\n",
    "axes[-1].set_xlabel('Time Step')\n",
    "axes[0].legend(fontsize=6, loc='best', ncol=2)\n",
    "fig.tight_layout()\n",
    "\n",
    "save_figure(fig, 'prediction_timeseries')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xmuewvehv8",
   "metadata": {},
   "source": [
    "## 9. Ergebnisse exportieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a2pkzdvr4",
   "metadata": {},
   "outputs": [],
   "source": "import json\n\nRESULTS_DIR = PROJECT_ROOT / 'results'\nRESULTS_DIR.mkdir(parents=True, exist_ok=True)\n\n# --- 1. Haupttabelle: alle Metriken (numerisch, kein Formatting) ---\nexport_metrics = metrics_df[['model', 'name']].copy()\nexport_metrics = export_metrics.merge(flops_df[['model', 'params']], on='model')\nexport_metrics = export_metrics.merge(\n    inference_df[['model', 'p95_ms', 'p95_std_ms']], on='model',\n)\n\n# Sample-level\nfor key in ['mse', 'rmse', 'mae', 'r2', 'accuracy']:\n    export_metrics[f'{key}_mean'] = metrics_df[f'{key}_mean']\n    export_metrics[f'{key}_std'] = metrics_df[f'{key}_std']\n\n# Sequence-level\nfor key in ['seq_rmse', 'seq_mae', 'seq_accuracy', 'seq_r2']:\n    export_metrics[f'{key}_mean'] = metrics_df[f'{key}_mean']\n    export_metrics[f'{key}_std'] = metrics_df[f'{key}_std']\n\nmetrics_path = RESULTS_DIR / 'eval_metrics.csv'\nexport_metrics.to_csv(metrics_path, index=False, float_format='%.6f')\nprint(f'Saved: {metrics_path.relative_to(PROJECT_ROOT)}')\n\n# --- 2. Inference-Zeiten ---\ninference_path = RESULTS_DIR / 'eval_inference.csv'\ninference_df.to_csv(inference_path, index=False, float_format='%.6f')\nprint(f'Saved: {inference_path.relative_to(PROJECT_ROOT)}')\n\n# --- 3. Attention Weight CSVs (pro Modell) ---\nfor model_id, profile in attention_profiles.items():\n    csv_path = FIGURES_DIR / f'attention_weights_{model_id.upper()}.csv'\n    attn_df = pd.DataFrame({\n        'timestep': np.arange(len(profile)),\n        'weight': profile,\n    })\n    attn_df.to_csv(csv_path, index=False, float_format='%.8f')\n    print(f'Saved: {csv_path.relative_to(PROJECT_ROOT)}')\n\n# --- 4. Statistische Ergebnisse (JSON) ---\nstats_export = {\n    'analysis_level': 'sequence',\n    'n_bootstrap': 1000,\n    'n_permutations': 10000,\n    'n_test_sequences': int(len(np.unique(test_sequence_ids))),\n    'accuracy_threshold': ACCURACY_THRESHOLD,\n    'bootstrap_results': {},\n    'comparisons': [],\n}\n\nfor mc in MODELS:\n    mid = mc.id.upper()\n    r = bootstrap_results[mc.id]\n    stats_export['bootstrap_results'][mid] = {\n        metric: {\n            'mean': r[metric]['mean'],\n            'std': r[metric]['std'],\n            'std_bootstrap': r[metric]['std_bootstrap'],\n            'std_seed': r[metric]['std_seed'],\n            'ci_lower': r[metric]['ci_lower'],\n            'ci_upper': r[metric]['ci_upper'],\n        }\n        for metric in METRICS\n    }\n\nfor c in comparison_results:\n    c_clean = {\n        'model_a': c['model_a'],\n        'model_b': c['model_b'],\n        'category': c['category'],\n        'n_sequences': c['n_sequences'],\n    }\n    for metric in METRICS:\n        c_clean[metric] = c[metric]\n    stats_export['comparisons'].append(c_clean)\n\nstats_path = RESULTS_DIR / 'eval_statistics.json'\nwith open(stats_path, 'w', encoding='utf-8') as f:\n    json.dump(stats_export, f, indent=2)\nprint(f'Saved: {stats_path.relative_to(PROJECT_ROOT)}')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}