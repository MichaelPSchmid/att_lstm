{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vollständige Evaluation — EPS Torque Prediction\n",
    "\n",
    "Alle Werte werden **aus den trainierten Checkpoints** berechnet. Keine vorberechneten `eval.json` als Input.\n",
    "\n",
    "| Schritt | Inhalt |\n",
    "|---------|--------|\n",
    "| 1 | Daten laden (einmalig) |\n",
    "| 2 | Checkpoint Discovery (8 Modelle × 5 Seeds) |\n",
    "| 3 | Inference — Predictions & Attention Weights |\n",
    "| 4 | Metriken (Sample-Level & Sequence-Level) |\n",
    "| 4b | Statistische Tests (Bootstrap, Permutationstests, Effektstärken) |\n",
    "| 5 | FLOPs & Parameter |\n",
    "| 6 | Inference Time Messung |\n",
    "| 7 | Ergebnis-Tabellen |\n",
    "| 8 | Figures (Attention, Tradeoff, Timeseries) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b1be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "import winsound\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Project root (notebooks/ -> project root)\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if not (PROJECT_ROOT / 'config').exists():\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from config.loader import load_config, get_model_class\n",
    "from config.settings import get_preprocessed_paths\n",
    "from model.data_module import TimeSeriesDataModule\n",
    "from scripts.shared import (\n",
    "    MODELS, MODEL_BY_ID,\n",
    "    find_all_seed_checkpoints,\n",
    "    calculate_metrics_dict,\n",
    "    aggregate_metrics_per_sequence,\n",
    ")\n",
    "from scripts.compute_sequence_r2 import compute_per_sequence_r2\n",
    "from scripts.evaluate_model import (\n",
    "    measure_inference_time, calculate_flops, has_attention_support,\n",
    ")\n",
    "\n",
    "# Constants\n",
    "SEEDS = [7, 42, 94, 123, 231]\n",
    "VARIANT = 'no_dropout'\n",
    "ACCURACY_THRESHOLD = 0.05\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f'Project root: {PROJECT_ROOT}')\n",
    "print(f'PyTorch:      {torch.__version__}')\n",
    "print(f'CUDA:         {torch.cuda.is_available()}')\n",
    "print(f'DEVICE:       {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc5e493",
   "metadata": {},
   "source": [
    "## 1. Daten laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29e9598e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading numpy file: C:\\Users\\MSchm\\Documents\\att_project\\data\\prepared_dataset\\HYUNDAI_SONATA_2020\\50_1_1_sF\\features_50_1_1_sF.npy\n",
      "Loading numpy file: C:\\Users\\MSchm\\Documents\\att_project\\data\\prepared_dataset\\HYUNDAI_SONATA_2020\\50_1_1_sF\\targets_50_1_1_sF.npy\n",
      "Loaded features: torch.Size([2201265, 50, 5])\n",
      "Loaded targets: torch.Size([2201265, 1])\n",
      "Loaded sequence_ids: 2201265 (4988 unique sequences)\n",
      "Sequence-level split (seed=0): 3491 train / 997 val / 500 test sequences\n",
      "Sample counts: 1539545 train / 440533 val / 221187 test\n",
      "\n",
      "Test samples:   221,187\n",
      "Test sequences: 500\n"
     ]
    }
   ],
   "source": [
    "config = load_config(str(PROJECT_ROOT / MODELS[0].config_no_dropout))\n",
    "data_config = config['data']\n",
    "\n",
    "paths = get_preprocessed_paths(\n",
    "    vehicle=data_config['vehicle'],\n",
    "    window_size=data_config['window_size'],\n",
    "    predict_size=data_config['predict_size'],\n",
    "    step_size=data_config['step_size'],\n",
    "    suffix='sF',\n",
    "    variant=data_config['variant'],\n",
    ")\n",
    "\n",
    "data_module = TimeSeriesDataModule(\n",
    "    feature_path=str(paths['features']),\n",
    "    target_path=str(paths['targets']),\n",
    "    sequence_ids_path=str(paths['sequence_ids']),\n",
    "    batch_size=256,\n",
    "    split_seed=data_config.get('split_seed', 0),\n",
    ")\n",
    "data_module.setup()\n",
    "\n",
    "# DataLoader with num_workers=0 (Windows/notebook compatibility)\n",
    "test_loader = DataLoader(\n",
    "    data_module.test_dataset, batch_size=256, shuffle=False, num_workers=0,\n",
    ")\n",
    "test_sequence_ids = data_module.get_split_sequence_ids('test')\n",
    "\n",
    "print(f'\\nTest samples:   {len(data_module.test_dataset):,}')\n",
    "print(f'Test sequences: {len(np.unique(test_sequence_ids)):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49564b00",
   "metadata": {},
   "source": [
    "## 2. Checkpoint Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba8e382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                                Seeds\n",
      "--------------------------------------------------\n",
      "  M1 MLP Last                       5/5\n",
      "  M2 MLP Flat                       5/5\n",
      "  M3 Small Baseline                 5/5\n",
      "  M4 Small + Simple Attn            5/5\n",
      "  M5 Medium Baseline                5/5\n",
      "  M6 Medium + Simple Attn           5/5\n",
      "  M7 Medium + Additive Attn         5/5\n",
      "  M8 Medium + Scaled DP             5/5\n",
      "\n",
      "Total: 40/40 checkpoints\n"
     ]
    }
   ],
   "source": [
    "all_checkpoints = {}\n",
    "\n",
    "print(f'{\"Model\":<35s} {\"Seeds\":>6s}')\n",
    "print('-' * 50)\n",
    "\n",
    "for mc in MODELS:\n",
    "    seed_ckpts = find_all_seed_checkpoints(mc, VARIANT)\n",
    "    all_checkpoints[mc.id] = seed_ckpts\n",
    "    found = len(seed_ckpts)\n",
    "    missing = [s for s in SEEDS if s not in seed_ckpts]\n",
    "    status = f'{found}/5'\n",
    "    if missing:\n",
    "        status += f'  (missing: {missing})'\n",
    "    print(f'  {mc.name:<33s} {status}')\n",
    "\n",
    "total = sum(len(v) for v in all_checkpoints.values())\n",
    "print(f'\\nTotal: {total}/40 checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac6cc1e",
   "metadata": {},
   "source": [
    "## 3. Inference — Predictions & Attention Weights\n",
    "\n",
    "Lädt jeden Checkpoint, berechnet Predictions auf dem Test-Set.\n",
    "Für Attention-Modelle (M4, M6, M7, M8) werden die Attention Weights\n",
    "gleichzeitig extrahiert und über alle Test-Samples gemittelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "234992be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, dataloader, device='cpu', extract_attention=False):\n",
    "    \"\"\"Run inference, optionally extracting attention weights.\n",
    "\n",
    "    Attention weights are averaged over samples incrementally to avoid\n",
    "    storing the full (N, seq_len, seq_len) matrix for additive attention.\n",
    "\n",
    "    Returns:\n",
    "        predictions, targets                       (if extract_attention=False)\n",
    "        predictions, targets, avg_attention_1d     (if extract_attention=True)\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds, all_targets = [], []\n",
    "    attn_sum = None\n",
    "    n_attn_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "\n",
    "            if extract_attention:\n",
    "                outputs, attn = model(X_batch, return_attention=True)\n",
    "                attn_np = attn.cpu().numpy()\n",
    "\n",
    "                if attn_np.ndim == 2:\n",
    "                    # Simple / Scaled DP: (batch, seq_len)\n",
    "                    batch_sum = attn_np.sum(axis=0)\n",
    "                elif attn_np.ndim == 3:\n",
    "                    # Additive (M7): (batch, query, key)\n",
    "                    # Average over query dim -> importance per key position\n",
    "                    batch_sum = attn_np.mean(axis=1).sum(axis=0)\n",
    "                else:\n",
    "                    batch_sum = np.zeros(50)\n",
    "\n",
    "                if attn_sum is None:\n",
    "                    attn_sum = batch_sum\n",
    "                else:\n",
    "                    attn_sum += batch_sum\n",
    "                n_attn_samples += len(attn_np)\n",
    "            else:\n",
    "                outputs = model(X_batch)\n",
    "\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_targets.append(Y_batch.numpy())\n",
    "\n",
    "    predictions = np.concatenate(all_preds, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    if extract_attention and attn_sum is not None:\n",
    "        avg_attention = attn_sum / n_attn_samples\n",
    "        return predictions, targets, avg_attention\n",
    "    return predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b2372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "M1 MLP Last:\n",
      "  Seed 7: val_loss=0.003477, samples=221,187\n",
      "  Seed 42: val_loss=0.003500, samples=221,187\n",
      "  Seed 94: val_loss=0.003454, samples=221,187\n",
      "  Seed 123: val_loss=0.003421, samples=221,187\n",
      "  Seed 231: val_loss=0.003470, samples=221,187\n",
      "\n",
      "M2 MLP Flat:\n",
      "  Seed 7: val_loss=0.002648, samples=221,187\n",
      "  Seed 42: val_loss=0.002598, samples=221,187\n",
      "  Seed 94: val_loss=0.002588, samples=221,187\n",
      "  Seed 123: val_loss=0.002775, samples=221,187\n",
      "  Seed 231: val_loss=0.002592, samples=221,187\n",
      "\n",
      "M3 Small Baseline:\n",
      "  Seed 7: val_loss=0.001940, samples=221,187\n",
      "  Seed 42: val_loss=0.001973, samples=221,187\n",
      "  Seed 94: val_loss=0.001993, samples=221,187\n",
      "  Seed 123: val_loss=0.001958, samples=221,187\n",
      "  Seed 231: val_loss=0.001933, samples=221,187\n",
      "\n",
      "M4 Small + Simple Attn:\n",
      "  Seed 7: val_loss=0.001993, samples=221,187\n",
      "  Seed 42: val_loss=0.001946, samples=221,187\n",
      "  Seed 94: val_loss=0.001987, samples=221,187\n",
      "  Seed 123: val_loss=0.001939, samples=221,187\n",
      "  Seed 231: val_loss=0.001964, samples=221,187\n",
      "\n",
      "M5 Medium Baseline:\n",
      "  Seed 7: val_loss=0.001952, samples=221,187\n",
      "  Seed 42: val_loss=0.001975, samples=221,187\n",
      "  Seed 94: val_loss=0.001965, samples=221,187\n",
      "  Seed 123: val_loss=0.001928, samples=221,187\n",
      "  Seed 231: val_loss=0.002002, samples=221,187\n",
      "\n",
      "M6 Medium + Simple Attn:\n",
      "  Seed 7: val_loss=0.001904, samples=221,187\n",
      "  Seed 42: val_loss=0.001909, samples=221,187\n",
      "  Seed 94: val_loss=0.001925, samples=221,187\n",
      "  Seed 123: val_loss=0.001928, samples=221,187\n",
      "  Seed 231: val_loss=0.001921, samples=221,187\n",
      "\n",
      "M7 Medium + Additive Attn:\n",
      "  Seed 7: val_loss=0.001907, samples=221,187\n",
      "  Seed 42: val_loss=0.001902, samples=221,187\n",
      "  Seed 94: val_loss=0.001929, samples=221,187\n",
      "  Seed 123: val_loss=0.001915, samples=221,187\n",
      "  Seed 231: val_loss=0.001943, samples=221,187\n",
      "\n",
      "M8 Medium + Scaled DP:\n",
      "  Seed 7: val_loss=0.001985, samples=221,187\n",
      "  Seed 42: val_loss=0.001998, samples=221,187\n",
      "  Seed 94: val_loss=0.001977, samples=221,187\n",
      "  Seed 123: val_loss=0.001994, samples=221,187\n",
      "  Seed 231: val_loss=0.001996, samples=221,187\n",
      "\n",
      "Done. 40 model-seed evaluations.\n"
     ]
    }
   ],
   "source": [
    "results = {}  # {model_id: {seed: {predictions, targets, attention}}}\n",
    "\n",
    "for mc in MODELS:\n",
    "    config_path = PROJECT_ROOT / mc.config_no_dropout\n",
    "    cfg = load_config(str(config_path))\n",
    "    model_class = get_model_class(cfg['model']['type'])\n",
    "\n",
    "    results[mc.id] = {}\n",
    "    seed_ckpts = all_checkpoints[mc.id]\n",
    "\n",
    "    print(f'\\n{mc.name}:')\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        if seed not in seed_ckpts:\n",
    "            print(f'  Seed {seed}: MISSING')\n",
    "            continue\n",
    "\n",
    "        ckpt_path, val_loss = seed_ckpts[seed]\n",
    "        model = model_class.load_from_checkpoint(str(ckpt_path), map_location='cpu')\n",
    "\n",
    "        has_attn = has_attention_support(model)\n",
    "\n",
    "        if has_attn:\n",
    "            preds, targs, attn = run_inference(\n",
    "                model, test_loader, DEVICE, extract_attention=True,\n",
    "            )\n",
    "        else:\n",
    "            preds, targs = run_inference(model, test_loader, DEVICE)\n",
    "            attn = None\n",
    "\n",
    "        results[mc.id][seed] = {\n",
    "            'predictions': preds,\n",
    "            'targets': targs,\n",
    "            'attention': attn,\n",
    "        }\n",
    "\n",
    "        print(f'  Seed {seed}: val_loss={val_loss:.6f}, samples={len(preds):,}')\n",
    "\n",
    "        del model\n",
    "        gc.collect()\n",
    "\n",
    "n_evals = sum(len(v) for v in results.values())\n",
    "print(f'\\nDone. {n_evals} model-seed evaluations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe5fcb7",
   "metadata": {},
   "source": [
    "## 4. Metriken berechnen\n",
    "\n",
    "- **Sample-Level:** MSE, RMSE, MAE, R², Accuracy\n",
    "- **Sequence-Level:** RMSE, MAE, Accuracy, R² (pro Sequenz, dann gemittelt)\n",
    "- Jeweils mean ± std über 5 Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1436bba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics computed.\n"
     ]
    }
   ],
   "source": [
    "metrics_rows = []\n",
    "\n",
    "for mc in MODELS:\n",
    "    seed_data = {\n",
    "        'mse': [], 'rmse': [], 'mae': [], 'r2': [], 'accuracy': [],\n",
    "        'seq_rmse': [], 'seq_mae': [], 'seq_accuracy': [], 'seq_r2': [],\n",
    "    }\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        if seed not in results[mc.id]:\n",
    "            continue\n",
    "\n",
    "        preds = results[mc.id][seed]['predictions']\n",
    "        targs = results[mc.id][seed]['targets']\n",
    "\n",
    "        # Sample-level metrics\n",
    "        m = calculate_metrics_dict(preds, targs, ACCURACY_THRESHOLD)\n",
    "        seed_data['mse'].append(m['mse'])\n",
    "        seed_data['rmse'].append(m['rmse'])\n",
    "        seed_data['mae'].append(m['mae'])\n",
    "        seed_data['r2'].append(m['r2'])\n",
    "        seed_data['accuracy'].append(m['accuracy'])\n",
    "\n",
    "        # Sequence-level metrics\n",
    "        _, seq_summary = aggregate_metrics_per_sequence(\n",
    "            preds, targs, test_sequence_ids, ACCURACY_THRESHOLD,\n",
    "        )\n",
    "        seed_data['seq_rmse'].append(seq_summary['rmse_mean'])\n",
    "        seed_data['seq_mae'].append(seq_summary['mae_mean'])\n",
    "        seed_data['seq_accuracy'].append(seq_summary['accuracy_mean'])\n",
    "\n",
    "        # Sequence-level R-squared\n",
    "        mean_r2, _ = compute_per_sequence_r2(preds, targs, test_sequence_ids)\n",
    "        seed_data['seq_r2'].append(mean_r2)\n",
    "\n",
    "    row = {'model': mc.id.upper(), 'name': mc.name}\n",
    "    for key, vals in seed_data.items():\n",
    "        if vals:\n",
    "            row[f'{key}_mean'] = np.mean(vals)\n",
    "            row[f'{key}_std'] = np.std(vals)\n",
    "    metrics_rows.append(row)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows)\n",
    "print('Metrics computed.')\n",
    "metrics_df[['model', 'name', 'accuracy_mean', 'accuracy_std',\n",
    "            'rmse_mean', 'rmse_std', 'seq_r2_mean', 'seq_r2_std']].round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8w258h2twsb",
   "metadata": {},
   "source": [
    "## 4b. Statistische Tests (Sequenz-Ebene)\n",
    "\n",
    "Block-Bootstrap CIs, Permutationstests und Effektstärken. Funktionen aus `scripts/sequence_level_evaluation.py` werden per Import wiederverwendet. Alle Berechnungen nutzen die vorhandenen Predictions im `results`-Dict — keine erneute Inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5oeo1aa6vdr",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.sequence_level_evaluation import (\n",
    "    bootstrap_ci_sequences,\n",
    "    cohens_d_paired_sequences,\n",
    "    permutation_test_sequences,\n",
    "    multi_seed_sequence_analysis,\n",
    "    run_all_comparisons,\n",
    "    _compute_seq_metric_arrays,\n",
    "    COMPARISON_PAIRS,\n",
    "    METRICS,\n",
    "    _significance_stars,\n",
    "    _effect_size_category,\n",
    ")\n",
    "\n",
    "# Compute per-sequence metric arrays for all models x seeds\n",
    "all_seq_metrics = {}  # {model_id: {seed: {metric: array}}}\n",
    "\n",
    "for mc in MODELS:\n",
    "    all_seq_metrics[mc.id] = {}\n",
    "    for seed in SEEDS:\n",
    "        if seed not in results[mc.id]:\n",
    "            continue\n",
    "        preds = results[mc.id][seed]['predictions']\n",
    "        targs = results[mc.id][seed]['targets']\n",
    "        seq_arrays = _compute_seq_metric_arrays(\n",
    "            preds, targs, test_sequence_ids, ACCURACY_THRESHOLD,\n",
    "        )\n",
    "        all_seq_metrics[mc.id][seed] = seq_arrays\n",
    "\n",
    "n_models = len(all_seq_metrics)\n",
    "n_total = sum(len(v) for v in all_seq_metrics.values())\n",
    "print(f'Per-sequence metrics computed: {n_models} models, {n_total} model-seed combinations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jlct02e7y5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block-Bootstrap CIs + Law of Total Variance for all models\n",
    "bootstrap_results = {}  # {model_id: aggregated multi-seed result}\n",
    "seed_stability = {}  # {model_id: per-seed accuracy values}\n",
    "\n",
    "for mc in MODELS:\n",
    "    model_id = mc.id\n",
    "    seed_data = all_seq_metrics[model_id]\n",
    "    seeds_available = sorted(seed_data.keys())\n",
    "\n",
    "    # Per-seed bootstrap CIs\n",
    "    seed_bootstrap_ci = {}\n",
    "    seed_point_metrics = {}\n",
    "\n",
    "    for seed in seeds_available:\n",
    "        seq_arrays = seed_data[seed]\n",
    "        point = {m: float(np.mean(seq_arrays[m])) for m in METRICS}\n",
    "        seed_point_metrics[seed] = point\n",
    "\n",
    "        ci = {}\n",
    "        for metric in METRICS:\n",
    "            ci[metric] = bootstrap_ci_sequences(\n",
    "                seq_arrays[metric], n_bootstrap=1000, seed=42,\n",
    "            )\n",
    "        seed_bootstrap_ci[seed] = ci\n",
    "\n",
    "    # Multi-seed aggregation (law of total variance)\n",
    "    aggregated = multi_seed_sequence_analysis(seed_bootstrap_ci, seed_point_metrics)\n",
    "    bootstrap_results[model_id] = aggregated\n",
    "    seed_stability[model_id] = {\n",
    "        'per_seed_accuracy': [seed_point_metrics[s]['accuracy'] for s in seeds_available],\n",
    "        'sigma_seed': aggregated['accuracy']['std_seed'],\n",
    "    }\n",
    "\n",
    "# --- Combined uncertainty table ---\n",
    "rows = []\n",
    "for mc in MODELS:\n",
    "    r = bootstrap_results[mc.id]\n",
    "    rows.append({\n",
    "        'Model': mc.id.upper(),\n",
    "        'Name': mc.name,\n",
    "        'Accuracy (%)': f\"{r['accuracy']['mean']:.2f} \\u00b1 {r['accuracy']['std']:.2f}\",\n",
    "        'RMSE': f\"{r['rmse']['mean']:.4f} \\u00b1 {r['rmse']['std']:.4f}\",\n",
    "        'MAE': f\"{r['mae']['mean']:.4f} \\u00b1 {r['mae']['std']:.4f}\",\n",
    "        'R\\u00b2': f\"{r['r2']['mean']:.3f} \\u00b1 {r['r2']['std']:.3f}\",\n",
    "    })\n",
    "print('Bootstrap CIs (combined uncertainty, 1000 samples \\u00d7 5 seeds):')\n",
    "display(pd.DataFrame(rows))\n",
    "\n",
    "# --- Seed stability ---\n",
    "print('\\nSeed Stability (per-seed accuracy):')\n",
    "stab_rows = []\n",
    "for mc in MODELS:\n",
    "    s = seed_stability[mc.id]\n",
    "    acc_vals = s['per_seed_accuracy']\n",
    "    row = {'Model': mc.id.upper()}\n",
    "    for seed, val in zip(SEEDS, acc_vals):\n",
    "        row[f'Seed {seed}'] = f'{val:.2f}'\n",
    "    row['\\u03c3_seed'] = f\"{s['sigma_seed']:.2f}\"\n",
    "    stab_rows.append(row)\n",
    "display(pd.DataFrame(stab_rows))\n",
    "\n",
    "# --- Uncertainty decomposition ---\n",
    "print('\\nUncertainty Decomposition (Law of Total Variance):')\n",
    "decomp_rows = []\n",
    "for mc in MODELS:\n",
    "    r = bootstrap_results[mc.id]\n",
    "    acc = r['accuracy']\n",
    "    sigma_total = acc['std']\n",
    "    sigma_boot = acc['std_bootstrap']\n",
    "    sigma_seed = acc['std_seed']\n",
    "    seed_frac = (sigma_seed**2 / sigma_total**2 * 100) if sigma_total > 0 else 0\n",
    "    decomp_rows.append({\n",
    "        'Model': mc.id.upper(),\n",
    "        '\\u03c3_total': f'{sigma_total:.2f}',\n",
    "        '\\u03c3_bootstrap': f'{sigma_boot:.2f}',\n",
    "        '\\u03c3_seed': f'{sigma_seed:.2f}',\n",
    "        'Seed Variance %': f'{seed_frac:.0f}%',\n",
    "    })\n",
    "display(pd.DataFrame(decomp_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leqnjczf1xe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed-averaged per-sequence metrics for pairwise comparisons\n",
    "avg_seq_metrics = {}  # {MODEL_ID: {metric: array}}\n",
    "\n",
    "for mc in MODELS:\n",
    "    model_id = mc.id.upper()\n",
    "    seed_data = all_seq_metrics[mc.id]\n",
    "    seeds_available = sorted(seed_data.keys())\n",
    "\n",
    "    avg = {}\n",
    "    for metric in METRICS:\n",
    "        avg[metric] = np.mean(\n",
    "            [seed_data[s][metric] for s in seeds_available], axis=0,\n",
    "        )\n",
    "    avg_seq_metrics[model_id] = avg\n",
    "\n",
    "# Run all pairwise comparisons (10,000 sign-flip permutations)\n",
    "comparison_results = run_all_comparisons(\n",
    "    COMPARISON_PAIRS, avg_seq_metrics,\n",
    "    n_permutations=10000, seed=42,\n",
    ")\n",
    "\n",
    "# Display results\n",
    "comp_rows = []\n",
    "for c in comparison_results:\n",
    "    acc = c['accuracy']\n",
    "    rmse = c['rmse']\n",
    "    mae = c['mae']\n",
    "    r2 = c['r2']\n",
    "    acc_stars = _significance_stars(acc['p_value'])\n",
    "    rmse_stars = _significance_stars(rmse['p_value'])\n",
    "    mae_stars = _significance_stars(mae['p_value'])\n",
    "    r2_stars = _significance_stars(r2['p_value'])\n",
    "\n",
    "    comp_rows.append({\n",
    "        'Comparison': f\"{c['model_a']} \\u2192 {c['model_b']}\",\n",
    "        'Category': c['category'],\n",
    "        '\\u0394 Acc (%)': f\"{acc['observed_diff']:+.2f}{acc_stars}\",\n",
    "        '\\u0394 RMSE': f\"{rmse['observed_diff']:+.4f}{rmse_stars}\",\n",
    "        '\\u0394 MAE': f\"{mae['observed_diff']:+.4f}{mae_stars}\",\n",
    "        '\\u0394 R\\u00b2': f\"{r2['observed_diff']:+.3f}{r2_stars}\",\n",
    "        'd(Acc)': f\"{acc['cohens_d']:+.3f}\",\n",
    "        'd(RMSE)': f\"{rmse['cohens_d']:+.3f}\",\n",
    "        'd(MAE)': f\"{mae['cohens_d']:+.3f}\",\n",
    "        'd(R\\u00b2)': f\"{r2['cohens_d']:+.3f}\",\n",
    "    })\n",
    "\n",
    "print(f'Pairwise Comparisons (10,000 permutations, seed=42):')\n",
    "print('Significance: * p<0.05, ** p<0.01, *** p<0.001')\n",
    "print(\"Cohen's d: |d|<0.2 negligible, 0.2-0.5 small, 0.5-0.8 medium, >0.8 large\")\n",
    "print(\"d sign: positive = B better (higher accuracy/R\\u00b2, lower RMSE/MAE)\\n\")\n",
    "pd.DataFrame(comp_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe45b26",
   "metadata": {},
   "source": [
    "## 5. FLOPs & Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = torch.randn(1, data_config['window_size'], 5)\n",
    "flops_rows = []\n",
    "\n",
    "for mc in MODELS:\n",
    "    cfg = load_config(str(PROJECT_ROOT / mc.config_no_dropout))\n",
    "    model_class = get_model_class(cfg['model']['type'])\n",
    "\n",
    "    seed_ckpts = all_checkpoints[mc.id]\n",
    "    any_seed = next(iter(seed_ckpts))\n",
    "    ckpt_path, _ = seed_ckpts[any_seed]\n",
    "    model = model_class.load_from_checkpoint(str(ckpt_path), map_location='cpu')\n",
    "\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    flops_result = calculate_flops(model, sample_input.clone())\n",
    "\n",
    "    flops_rows.append({\n",
    "        'model': mc.id.upper(),\n",
    "        'params': n_params,\n",
    "        'flops': flops_result['flops'],\n",
    "        'flops_fmt': flops_result['flops_formatted'],\n",
    "        'macs_fmt': flops_result['macs_formatted'],\n",
    "    })\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "flops_df = pd.DataFrame(flops_rows)\n",
    "flops_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11bb2f5",
   "metadata": {},
   "source": [
    "## 6. Inference Time Messung\n",
    "\n",
    "> **Hinweis:** Diese Zelle isoliert ausführen. Keine anderen rechenintensiven Prozesse parallel laufen lassen.\n",
    ">\n",
    "> Messung: CPU, single-thread (`torch.set_num_threads(1)`), 100 Warmup, 5 Runs × 1000 Samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d1540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cached inference times if available (measurement requires CPU)\n",
    "_inference_csv = PROJECT_ROOT / 'results' / 'eval_inference.csv'\n",
    "\n",
    "if _inference_csv.exists():\n",
    "    inference_df = pd.read_csv(_inference_csv)\n",
    "    print(f'Loaded cached inference times from {_inference_csv.relative_to(PROJECT_ROOT)}')\n",
    "    print('(To re-measure, delete the CSV and re-run with DEVICE=cpu)')\n",
    "    display(inference_df)\n",
    "else:\n",
    "    if DEVICE != 'cpu':\n",
    "        raise RuntimeError(\n",
    "            f'No cached inference times found at {_inference_csv} '\n",
    "            f'and DEVICE={DEVICE}. Inference timing requires CPU. '\n",
    "            f'Set DEVICE=\"cpu\" or provide the CSV.'\n",
    "        )\n",
    "\n",
    "    inference_rows = []\n",
    "\n",
    "    for mc in MODELS:\n",
    "        cfg = load_config(str(PROJECT_ROOT / mc.config_no_dropout))\n",
    "        model_class = get_model_class(cfg['model']['type'])\n",
    "\n",
    "        seed_ckpts = all_checkpoints[mc.id]\n",
    "        ckpt_path, _ = seed_ckpts[42]\n",
    "        model = model_class.load_from_checkpoint(str(ckpt_path), map_location='cpu')\n",
    "\n",
    "        timing = measure_inference_time(\n",
    "            model, sample_input.clone(),\n",
    "            warmup_iterations=100,\n",
    "            num_samples=1000,\n",
    "            num_runs=5,\n",
    "            device='cpu',\n",
    "        )\n",
    "\n",
    "        inference_rows.append({\n",
    "            'model': mc.id.upper(),\n",
    "            'mean_ms': timing['mean_ms'],\n",
    "            'std_ms': timing['std_ms'],\n",
    "            'p50_ms': timing['p50_ms'],\n",
    "            'p50_std_ms': timing['p50_std_ms'],\n",
    "            'p95_ms': timing['p95_ms'],\n",
    "            'p95_std_ms': timing['p95_std_ms'],\n",
    "            'p99_ms': timing['p99_ms'],\n",
    "            'p99_std_ms': timing['p99_std_ms'],\n",
    "        })\n",
    "\n",
    "        print(f'  {mc.name:<35s}  P95={timing[\"p95_ms\"]:.3f} +/- {timing[\"p95_std_ms\"]:.3f} ms')\n",
    "\n",
    "        del model\n",
    "        gc.collect()\n",
    "\n",
    "    inference_df = pd.DataFrame(inference_rows)\n",
    "    display(inference_df)\n",
    "    winsound.PlaySound(\"SystemHand\", winsound.SND_ALIAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32de7878",
   "metadata": {},
   "source": [
    "## 7. Ergebnis-Tabellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7530258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt(row, key, decimals=4):\n",
    "    \"\"\"Format mean +/- std.\"\"\"\n",
    "    m = row.get(f'{key}_mean', float('nan'))\n",
    "    s = row.get(f'{key}_std', float('nan'))\n",
    "    return f'{m:.{decimals}f} +/- {s:.{decimals}f}'\n",
    "\n",
    "\n",
    "main = metrics_df[['model', 'name']].copy()\n",
    "main = main.merge(flops_df[['model', 'params']], on='model')\n",
    "main = main.merge(inference_df[['model', 'p95_ms', 'p95_std_ms']], on='model')\n",
    "\n",
    "main['Accuracy (%)'] = metrics_df.apply(lambda r: fmt(r, 'accuracy', 2), axis=1)\n",
    "main['RMSE'] = metrics_df.apply(lambda r: fmt(r, 'rmse'), axis=1)\n",
    "main['MAE'] = metrics_df.apply(lambda r: fmt(r, 'mae'), axis=1)\n",
    "main['R2 (sample)'] = metrics_df.apply(lambda r: fmt(r, 'r2'), axis=1)\n",
    "main['R2 (sequence)'] = metrics_df.apply(lambda r: fmt(r, 'seq_r2'), axis=1)\n",
    "main['Seq Accuracy (%)'] = metrics_df.apply(lambda r: fmt(r, 'seq_accuracy', 2), axis=1)\n",
    "main['Seq RMSE'] = metrics_df.apply(lambda r: fmt(r, 'seq_rmse'), axis=1)\n",
    "main['P95 (ms)'] = main.apply(\n",
    "    lambda r: f\"{r['p95_ms']:.3f} +/- {r['p95_std_ms']:.3f}\", axis=1,\n",
    ")\n",
    "\n",
    "display_cols = [\n",
    "    'model', 'name', 'params',\n",
    "    'Accuracy (%)', 'RMSE', 'MAE',\n",
    "    'R2 (sample)', 'R2 (sequence)',\n",
    "    'Seq Accuracy (%)', 'Seq RMSE',\n",
    "    'P95 (ms)',\n",
    "]\n",
    "main[display_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0015ceca",
   "metadata": {},
   "source": [
    "## 8. Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b744599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import scienceplots  # noqa: F401\n",
    "    plt.style.use(['science', 'ieee'])\n",
    "except ImportError:\n",
    "    print('WARNING: scienceplots not installed, using default style')\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'text.usetex': True,\n",
    "    'pgf.texsystem': 'pdflatex',\n",
    "    'pgf.rcfonts': False,\n",
    "    'pgf.preamble': '\\n'.join([\n",
    "        r'\\usepackage[utf8]{inputenc}',\n",
    "        r'\\usepackage[T1]{fontenc}',\n",
    "        r'\\usepackage{amsmath}',\n",
    "        r'\\usepackage{siunitx}',\n",
    "        r'\\providecommand{\\mathdefault}[1]{#1}',\n",
    "    ]),\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Computer Modern Roman'],\n",
    "    'figure.figsize': (3.5, 2.5),\n",
    "    'savefig.bbox': 'tight',\n",
    "    'savefig.pad_inches': 0.02,\n",
    "    'lines.linewidth': 1.0,\n",
    "    'lines.markersize': 4,\n",
    "    'axes.grid': False,\n",
    "    'legend.framealpha': 0.95,\n",
    "    'legend.edgecolor': 'none',\n",
    "    'savefig.dpi': 300,\n",
    "})\n",
    "\n",
    "MATLAB_BLUE = (0/255, 114/255, 189/255)\n",
    "MATLAB_ORANGE = (217/255, 83/255, 25/255)\n",
    "MATLAB_PURPLE = (126/255, 47/255, 142/255)\n",
    "\n",
    "FIGURES_DIR = PROJECT_ROOT / 'figures'\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_figure(fig, name):\n",
    "    \"\"\"Save figure as PGF, PDF, and PNG.\"\"\"\n",
    "    for ext in ['pgf', 'pdf', 'png']:\n",
    "        path = FIGURES_DIR / f'{name}.{ext}'\n",
    "        fig.savefig(path)\n",
    "        print(f'  Saved: {path.relative_to(PROJECT_ROOT)}')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "print('Figure styling configured.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d0bf76",
   "metadata": {},
   "source": [
    "### 8a. Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e345351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average attention profiles across seeds for each attention model\n",
    "attention_profiles = {}\n",
    "\n",
    "for model_id in ['m4', 'm6', 'm7', 'm8']:\n",
    "    seed_weights = []\n",
    "    for seed in SEEDS:\n",
    "        if seed not in results[model_id]:\n",
    "            continue\n",
    "        attn = results[model_id][seed]['attention']\n",
    "        if attn is None:\n",
    "            continue\n",
    "        seed_weights.append(attn)\n",
    "\n",
    "    if not seed_weights:\n",
    "        print(f'  {model_id.upper()}: no attention data')\n",
    "        continue\n",
    "\n",
    "    combined = np.mean(seed_weights, axis=0)\n",
    "    combined = combined / combined.sum()  # Normalize to sum=1\n",
    "    attention_profiles[model_id] = combined\n",
    "\n",
    "    last5 = combined[-5:].sum() * 100\n",
    "    last10 = combined[-10:].sum() * 100\n",
    "    last20 = combined[-20:].sum() * 100\n",
    "    peak = np.argmax(combined)\n",
    "    print(f'  {model_id.upper()}: Last5={last5:.1f}%, Last10={last10:.1f}%, '\n",
    "          f'Last20={last20:.1f}%, Peak={peak}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93e8f2b",
   "metadata": {},
   "outputs": [],
   "source": "# Plot: Combined attention weights (M6, M7, M8) in single figure\nMATLAB_BLUE = (0/255, 114/255, 189/255)\nMATLAB_ORANGE = (217/255, 83/255, 25/255)\nMATLAB_PURPLE = (126/255, 47/255, 142/255)\n\nmodels_to_plot = [\n    ('m6', 'Simple (M6)', MATLAB_BLUE, '-'),\n    ('m7', 'Additive (M7)', MATLAB_ORANGE, '--'),\n    ('m8', 'Scaled Dot-Product (M8)', MATLAB_PURPLE, ':'),\n]\n\nfig, ax = plt.subplots(figsize=(3.5, 2.5))\n\nfor mid, label, color, ls in models_to_plot:\n    if mid not in attention_profiles:\n        continue\n\n    w = attention_profiles[mid]\n    t = np.arange(len(w))\n    ax.plot(t, w, color=color, linewidth=1.2, linestyle=ls, label=label)\n\nax.axhline(\n    y=1.0 / 50, color='gray', linestyle='--', linewidth=0.7,\n    label=r'Uniform ($\\frac{1}{50}$)',\n)\n\nax.set_xlabel('Time Step')\nax.set_ylabel('Attention Weight')\nax.set_xlim(0, 49)\nax.set_ylim(bottom=0)\nax.legend(fontsize=6, loc='upper left')\n\nsave_figure(fig, 'fig_attention_comparison')"
  },
  {
   "cell_type": "markdown",
   "id": "965cc266",
   "metadata": {},
   "source": [
    "### 8b. Inference-MAE Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09e179f",
   "metadata": {},
   "outputs": [],
   "source": "# Build data from computed metrics + inference times\ntradeoff_data = {}\nfor _, mrow in metrics_df.iterrows():\n    mid = mrow['model']\n    irow = inference_df[inference_df['model'] == mid].iloc[0]\n\n    if mid in ('M1', 'M2'):\n        mtype = 'mlp'\n    elif mid in ('M3', 'M5'):\n        mtype = 'lstm'\n    else:\n        mtype = 'lstm_attn'\n\n    tradeoff_data[mid] = {\n        'mae': mrow['seq_mae_mean'],\n        'p95_ms': irow['p95_ms'],\n        'type': mtype,\n    }\n\n# Marker and color maps\nMARKERS = {'mlp': 's', 'lstm': 'o', 'lstm_attn': '^'}\nTYPE_COLORS = {'mlp': '#7f7f7f', 'lstm': '#1f77b4', 'lstm_attn': '#ff7f0e'}\nMARKER_SIZES = {'mlp': 35, 'lstm': 45, 'lstm_attn': 50}\n\nfig, ax = plt.subplots(figsize=(3.5, 2.8))\n\nplotted_types = set()\nfor mid, d in tradeoff_data.items():\n    mtype = d['type']\n    label = None\n    if mtype not in plotted_types:\n        label = {'mlp': 'MLP Baseline', 'lstm': 'LSTM Baseline',\n                 'lstm_attn': 'LSTM + Attention'}[mtype]\n        plotted_types.add(mtype)\n\n    ax.scatter(\n        d['p95_ms'], d['mae'],\n        marker=MARKERS[mtype], s=MARKER_SIZES[mtype],\n        c=TYPE_COLORS[mtype], edgecolors='white', linewidths=0.5,\n        label=label, zorder=4,\n    )\n\n# Model labels: offset-based; M5/M6/M8 fan out with short leader lines\n_arrow = dict(arrowstyle='-', color='0.4', lw=0.5, shrinkB=3)\n\n#                  (x_off, y_off, ha,       va,       arrow?)\n_lbl = {\n    'M1': (  5,  -3, 'left',   'top',    False),\n    'M2': (  5,  -3, 'left',   'top',    False),\n    'M3': (  0,   8, 'center', 'bottom', False),\n    'M4': (  0,  -8, 'center', 'top',    False),\n    'M5': ( -8,  20, 'right',  'bottom', True),   # above-left\n    'M6': (  0, -16, 'center', 'top',    True),   # below\n    'M8': (  8,  16, 'left',   'bottom', True),   # above-right\n    'M7': (  5,   3, 'left',   'bottom', False),\n}\n\nfor mid, d in tradeoff_data.items():\n    xo, yo, ha, va, use_arrow = _lbl[mid]\n    ax.annotate(\n        mid, (d['p95_ms'], d['mae']),\n        xytext=(xo, yo), textcoords='offset points',\n        fontsize=7, ha=ha, va=va,\n        arrowprops=_arrow if use_arrow else None,\n    )\n\nax.set_xscale('log')\nax.set_xlim(0.04, 7)\nax.set_ylim(0.030, None)\nax.set_xticks([0.05, 0.1, 0.5, 1, 2, 5])\nax.set_xticklabels(['0.05', '0.1', '0.5', '1', '2', '5'])\nax.set_xlabel(r'Inference Time P95 (ms)')\nax.set_ylabel(r'MAE')\nax.legend(loc='upper right', fontsize=7)\n\nsave_figure(fig, 'fig_inference_mae_tradeoff')"
  },
  {
   "cell_type": "markdown",
   "id": "c7b85bf0",
   "metadata": {},
   "source": [
    "### 8c. Prediction Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3a135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use seed 42, models M2/M3/M5/M7\n",
    "PRED_MODELS = ['m2', 'm3', 'm5', 'm7']\n",
    "PRED_SEED = 42\n",
    "\n",
    "# Compute per-sequence RMSE using M5 as reference\n",
    "ref_preds = results['m5'][PRED_SEED]['predictions'].flatten()\n",
    "ref_targs = results['m5'][PRED_SEED]['targets'].flatten()\n",
    "\n",
    "unique_seqs = np.unique(test_sequence_ids)\n",
    "seq_rmse = {}\n",
    "for sid in unique_seqs:\n",
    "    mask = test_sequence_ids == sid\n",
    "    p = ref_preds[mask]\n",
    "    t = ref_targs[mask]\n",
    "    seq_rmse[sid] = np.sqrt(np.mean((p - t) ** 2))\n",
    "\n",
    "rmse_values = np.array(list(seq_rmse.values()))\n",
    "seq_ids_arr = np.array(list(seq_rmse.keys()))\n",
    "\n",
    "\n",
    "def pick_sequence(lo_pct, hi_pct):\n",
    "    \"\"\"Pick sequence with RMSE closest to midpoint of percentile range.\"\"\"\n",
    "    lo = np.percentile(rmse_values, lo_pct)\n",
    "    hi = np.percentile(rmse_values, hi_pct)\n",
    "    mask = (rmse_values >= lo) & (rmse_values <= hi)\n",
    "    if not mask.any():\n",
    "        mid = np.percentile(rmse_values, (lo_pct + hi_pct) / 2)\n",
    "        idx = np.argmin(np.abs(rmse_values - mid))\n",
    "        return seq_ids_arr[idx]\n",
    "    mid = (lo + hi) / 2\n",
    "    candidates = rmse_values[mask]\n",
    "    cand_ids = seq_ids_arr[mask]\n",
    "    best = np.argmin(np.abs(candidates - mid))\n",
    "    return cand_ids[best]\n",
    "\n",
    "\n",
    "selected = {\n",
    "    'good': pick_sequence(10, 25),\n",
    "    'median': pick_sequence(45, 55),\n",
    "    'difficult': pick_sequence(75, 90),\n",
    "}\n",
    "\n",
    "for label, sid in selected.items():\n",
    "    print(f'  {label}: seq_id={sid}, RMSE={seq_rmse[sid]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cc5417",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    ('good', 'Good Prediction'),\n",
    "    ('median', 'Median Prediction'),\n",
    "    ('difficult', 'Difficult Prediction'),\n",
    "]\n",
    "\n",
    "model_colors = {\n",
    "    'm2': '#7f7f7f', 'm3': MATLAB_BLUE,\n",
    "    'm5': MATLAB_ORANGE, 'm7': MATLAB_PURPLE,\n",
    "}\n",
    "model_styles = {'m2': ':', 'm3': '--', 'm5': '-.', 'm7': (0, (3, 1, 1, 1))}\n",
    "model_labels = {\n",
    "    'm2': 'M2 (MLP)', 'm3': 'M3 (Small)',\n",
    "    'm5': 'M5 (Medium)', 'm7': 'M7 (+ Attn)',\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(3.5, 5.5), sharex=True)\n",
    "\n",
    "for ax, (cat, title_prefix) in zip(axes, categories):\n",
    "    sid = selected[cat]\n",
    "    mask = test_sequence_ids == sid\n",
    "\n",
    "    gt = results['m5'][PRED_SEED]['targets'].flatten()[mask]\n",
    "    timesteps = np.arange(len(gt))\n",
    "\n",
    "    ax.plot(\n",
    "        timesteps, gt, color='black', linewidth=1.0,\n",
    "        linestyle='-', label='Ground Truth',\n",
    "    )\n",
    "\n",
    "    for mid in PRED_MODELS:\n",
    "        p = results[mid][PRED_SEED]['predictions'].flatten()[mask]\n",
    "        ax.plot(\n",
    "            timesteps, p, color=model_colors[mid], linewidth=0.8,\n",
    "            linestyle=model_styles[mid], label=model_labels[mid],\n",
    "        )\n",
    "\n",
    "    rmse_m5 = seq_rmse[sid]\n",
    "    ax.set_title(\n",
    "        f'{title_prefix} (RMSE$_{{\\\\mathrm{{M5}}}}={rmse_m5:.3f}$)',\n",
    "        fontsize=8,\n",
    "    )\n",
    "    ax.set_ylabel('Torque (norm.)')\n",
    "\n",
    "axes[-1].set_xlabel('Time Step')\n",
    "axes[0].legend(fontsize=6, loc='best', ncol=3)\n",
    "fig.tight_layout()\n",
    "\n",
    "save_figure(fig, 'fig_prediction_timeseries')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xmuewvehv8",
   "metadata": {},
   "source": [
    "## 9. Ergebnisse exportieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a2pkzdvr4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PROJECT_ROOT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m RESULTS_DIR = \u001b[43mPROJECT_ROOT\u001b[49m / \u001b[33m'\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m RESULTS_DIR.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# --- 1. Haupttabelle: alle Metriken (numerisch, kein Formatting) ---\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'PROJECT_ROOT' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "RESULTS_DIR = PROJECT_ROOT / 'results'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- 1. Haupttabelle: alle Metriken (numerisch, kein Formatting) ---\n",
    "export_metrics = metrics_df[['model', 'name']].copy()\n",
    "export_metrics = export_metrics.merge(flops_df[['model', 'params']], on='model')\n",
    "export_metrics = export_metrics.merge(\n",
    "    inference_df[['model', 'p95_ms', 'p95_std_ms']], on='model',\n",
    ")\n",
    "\n",
    "# Sample-level\n",
    "for key in ['mse', 'rmse', 'mae', 'r2', 'accuracy']:\n",
    "    export_metrics[f'{key}_mean'] = metrics_df[f'{key}_mean']\n",
    "    export_metrics[f'{key}_std'] = metrics_df[f'{key}_std']\n",
    "\n",
    "# Sequence-level\n",
    "for key in ['seq_rmse', 'seq_mae', 'seq_accuracy', 'seq_r2']:\n",
    "    export_metrics[f'{key}_mean'] = metrics_df[f'{key}_mean']\n",
    "    export_metrics[f'{key}_std'] = metrics_df[f'{key}_std']\n",
    "\n",
    "metrics_path = RESULTS_DIR / 'eval_metrics.csv'\n",
    "export_metrics.to_csv(metrics_path, index=False, float_format='%.6f')\n",
    "print(f'Saved: {metrics_path.relative_to(PROJECT_ROOT)}')\n",
    "\n",
    "# --- 2. Inference-Zeiten ---\n",
    "inference_path = RESULTS_DIR / 'eval_inference.csv'\n",
    "inference_df.to_csv(inference_path, index=False, float_format='%.6f')\n",
    "print(f'Saved: {inference_path.relative_to(PROJECT_ROOT)}')\n",
    "\n",
    "# --- 3. Attention Weight CSVs (pro Modell) ---\n",
    "for model_id, profile in attention_profiles.items():\n",
    "    csv_path = FIGURES_DIR / f'attention_weights_{model_id.upper()}.csv'\n",
    "    attn_df = pd.DataFrame({\n",
    "        'timestep': np.arange(len(profile)),\n",
    "        'weight': profile,\n",
    "    })\n",
    "    attn_df.to_csv(csv_path, index=False, float_format='%.8f')\n",
    "    print(f'Saved: {csv_path.relative_to(PROJECT_ROOT)}')\n",
    "\n",
    "# --- 4. Statistische Ergebnisse (JSON) ---\n",
    "stats_export = {\n",
    "    'analysis_level': 'sequence',\n",
    "    'n_bootstrap': 1000,\n",
    "    'n_permutations': 10000,\n",
    "    'n_test_sequences': int(len(np.unique(test_sequence_ids))),\n",
    "    'accuracy_threshold': ACCURACY_THRESHOLD,\n",
    "    'bootstrap_results': {},\n",
    "    'comparisons': [],\n",
    "}\n",
    "\n",
    "for mc in MODELS:\n",
    "    mid = mc.id.upper()\n",
    "    r = bootstrap_results[mc.id]\n",
    "    stats_export['bootstrap_results'][mid] = {\n",
    "        metric: {\n",
    "            'mean': r[metric]['mean'],\n",
    "            'std': r[metric]['std'],\n",
    "            'std_bootstrap': r[metric]['std_bootstrap'],\n",
    "            'std_seed': r[metric]['std_seed'],\n",
    "            'ci_lower': r[metric]['ci_lower'],\n",
    "            'ci_upper': r[metric]['ci_upper'],\n",
    "        }\n",
    "        for metric in METRICS\n",
    "    }\n",
    "\n",
    "for c in comparison_results:\n",
    "    c_clean = {\n",
    "        'model_a': c['model_a'],\n",
    "        'model_b': c['model_b'],\n",
    "        'category': c['category'],\n",
    "        'n_sequences': c['n_sequences'],\n",
    "    }\n",
    "    for metric in METRICS:\n",
    "        c_clean[metric] = c[metric]\n",
    "    stats_export['comparisons'].append(c_clean)\n",
    "\n",
    "stats_path = RESULTS_DIR / 'eval_statistics.json'\n",
    "with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(stats_export, f, indent=2)\n",
    "print(f'Saved: {stats_path.relative_to(PROJECT_ROOT)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50dce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "winsound.PlaySound(\"SystemHand\", winsound.SND_ALIAS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}