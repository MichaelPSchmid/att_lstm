# Training Session - 2026-01-21/22/23

## Ablation Study: Dropout-Vergleich

### Config-Matrix (12 Configs)

| Modell | Ohne Dropout | Mit Dropout | Status |
|--------|--------------|-------------|--------|
| M1 Small Baseline | `m1_small_baseline.yaml` | `m1_small_baseline_dropout.yaml` | ✅ / ⏳ |
| M2 Small + Simple Attn | `m2_small_simple_attn.yaml` | `m2_small_simple_attn_dropout.yaml` | ✅ / ⏳ |
| M3 Medium Baseline | `m3_medium_baseline.yaml` | `m3_medium_baseline_dropout.yaml` | ✅ / ⏳ |
| M4 Medium + Simple Attn | `m4_medium_simple_attn.yaml` | `m4_medium_simple_attn_dropout.yaml` | ✅ / ⏳ |
| M5 Medium + Additive Attn | `m5_medium_additive_attn.yaml` | `m5_medium_additive_attn_dropout.yaml` | ✅ / ⏳ |
| M6 Medium + Scaled DP Attn | `m6_medium_scaled_dp_attn.yaml` | `m6_medium_scaled_dp_attn_dropout.yaml` | ✅ / ⏳ |

### Namenskonvention

```
Basis-Config (kein Suffix) = dropout: 0.0, patience: 5
_dropout Suffix            = dropout: 0.2, patience: 15
```

### Hyperparameter-Settings

| Setting | Ohne Dropout | Mit Dropout |
|---------|--------------|-------------|
| dropout | 0.0 | 0.2 |
| patience | 5 | 15 |
| LR (Small) | 0.001 | 0.001 |
| LR (Medium) | 0.0005 | 0.0005 |

---

## Bisherige Ergebnisse

### M1 Small Baseline (Archiv)

| Konfiguration | Test Accuracy | Test R² | val_loss | Epochen |
|---------------|---------------|---------|----------|---------|
| Ohne Dropout | **82.5%** | **0.860** | 0.0017 | ~14 |
| Mit Dropout 0.2 | ~80.5% | ~0.843 | 0.00188 | ~50+ |

**Erkenntnis:** Dropout verursacht Underfitting bei kleinen Modellen (~85K Parameter).

---

## Trainingskommandos

### Ohne Dropout (Basis)
```bash
python scripts/train_model.py --config config/model_configs/m1_small_baseline.yaml
python scripts/train_model.py --config config/model_configs/m2_small_simple_attn.yaml
python scripts/train_model.py --config config/model_configs/m3_medium_baseline.yaml
python scripts/train_model.py --config config/model_configs/m4_medium_simple_attn.yaml
python scripts/train_model.py --config config/model_configs/m5_medium_additive_attn.yaml
python scripts/train_model.py --config config/model_configs/m6_medium_scaled_dp_attn.yaml
```

### Mit Dropout (Ablation)
```bash
python scripts/train_model.py --config config/model_configs/m1_small_baseline_dropout.yaml
python scripts/train_model.py --config config/model_configs/m2_small_simple_attn_dropout.yaml
python scripts/train_model.py --config config/model_configs/m3_medium_baseline_dropout.yaml
python scripts/train_model.py --config config/model_configs/m4_medium_simple_attn_dropout.yaml
python scripts/train_model.py --config config/model_configs/m5_medium_additive_attn_dropout.yaml
python scripts/train_model.py --config config/model_configs/m6_medium_scaled_dp_attn_dropout.yaml
```

---

## Paper-Dokumentation

### Empirische Begründung für unterschiedliche Settings

**Analyse der Loss-Kurven (M1):** Train- und Val-Loss liefen parallel → kein Overfitting.
Der vermeintliche "Einbruch" war durch zu hohe LR (0.005) verursacht, nicht durch Overfitting.

### Paper-Formulierung (Entwurf)

> "To ensure fair comparison, we conducted an ablation study on dropout regularization.
> Each model was trained both with and without dropout (0.2). Results showed that
> the effect of dropout varies by model size: smaller models (~85K parameters)
> achieved better performance without dropout, while larger models (~600K parameters)
> [results pending]. All hyperparameters except dropout were kept constant within
> each model size category."

### Ablation-Tabelle für Paper

| Model | Size | Dropout | Accuracy | R² | MSE | Inference | Bemerkung |
|-------|------|---------|----------|-----|-----|-----------|-----------|
| M1 Baseline | Small | 0.0 | **82.54%** | **0.860** | 0.00167 | 1.13ms | ✅ |
| M1 Baseline | Small | 0.2 | 80.5% | 0.84 | - | - | ✅ Archiv |
| M2 Simple Attn | Small | 0.0 | 81.91% | 0.854 | 0.00175 | 1.05ms | ✅ |
| M2 Simple Attn | Small | 0.2 | ? | ? | ? | ? | ⏳ TODO |
| M3 Baseline | Medium | 0.0 | 87.81% | 0.903 | 0.00116 | 2.14ms | ✅ |
| M3 Baseline | Medium | 0.2 | ? | ? | ? | ? | ⏳ TODO |
| **M4 Simple Attn** | Medium | 0.0 | **90.17%** | **0.918** | **0.00098** | 2.28ms | ✅ **BEST** |
| M4 Simple Attn | Medium | 0.2 | ? | ? | ? | ? | ⏳ TODO |
| M5 Additive Attn | Medium | 0.0 | 88.35% | 0.907 | 0.00111 | 2.64ms | ✅ |
| M5 Additive Attn | Medium | 0.2 | ? | ? | ? | ? | ⏳ TODO |
| M6 Scaled DP Attn | Medium | 0.0 | 89.80% | 0.916 | 0.00100 | 2.27ms | ✅ |
| M6 Scaled DP Attn | Medium | 0.2 | ? | ? | ? | ? | ⏳ TODO |

---

## Erkenntnisse (2026-01-23)

### Small Models (~85K Parameter)
- Attention hilft **NICHT** bei kleinen Modellen
- M1 Baseline (82.54%) > M2 Simple Attn (81.91%)
- Vermutung: Zu wenig Kapazität für sinnvolle Attention-Gewichte

### Medium Models (~600K Parameter)
- Attention hilft **signifikant**!
- Ranking: **M4 Simple (90.17%)** > M6 Scaled DP (89.80%) > M5 Additive (88.35%) > M3 Baseline (87.81%)
- Alle Attention-Varianten schlagen die Baseline (+0.5% bis +2.4%)

### Attention-Mechanismen Vergleich (Medium)
1. **Simple Attention:** Bestes Ergebnis, einfachste Implementierung
2. **Scaled Dot-Product:** Sehr nah an Simple, theoretisch fundiert
3. **Additive (Bahdanau):** Schlechteste der drei, mehr Parameter

---

## Nächste Schritte

1. [x] Alle 6 Modelle ohne Dropout trainieren
2. [x] Ergebnisse in Tabelle eintragen (2026-01-23)
3. [ ] Alle 6 Modelle mit Dropout trainieren
4. [ ] Dropout vs. No-Dropout vergleichen
5. [ ] Paper-Ablation-Tabelle finalisieren

---

## Ergebnis-Dateien

- `results/eval_m1_no_dropout.json`
- `results/eval_m2_no_dropout.json`
- `results/eval_m3_no_dropout.json`
- `results/eval_m4_no_dropout.json`
- `results/eval_m5_no_dropout.json`
- `results/eval_m6_no_dropout.json`
- Attention Heatmaps: `results/figures/M{2,4,5,6}_*/`
