# Training Session - 2026-01-21

## Status

| Modell | Config | Status | RÂ² | Accuracy |
|--------|--------|--------|-----|----------|
| M1 Small Baseline | m1_small_baseline.yaml | ðŸ”„ Training (neu) | - | - |
| M2 Small + Simple Attention | m2_small_simple_attn.yaml | â³ Queued | - | - |
| M3 Medium Baseline | m3_medium_baseline.yaml | â³ Queued | - | - |
| M4 Medium + Simple Attention | m4_medium_simple_attn.yaml | â³ Queued | - | - |
| M5 Medium + Additive Attention | m5_medium_additive_attn.yaml | â³ Queued | - | - |
| M6 Medium + Scaled DP Attention | m6_medium_scaled_dp_attn.yaml | â³ Queued | - | - |

## Hyperparameter-Entscheidungen (Paper-relevant!)

### Small vs Medium Modelle - unterschiedliche Konfiguration

| Setting | M1/M2 (Small) | M3-M6 (Medium) | BegrÃ¼ndung |
|---------|---------------|----------------|------------|
| dropout | **0.0** | 0.2 | Small-Modelle zeigen kein Overfitting |
| patience | **5** | 15 | Schnellere Konvergenz ohne Dropout |
| LR | 0.001 | 0.0005 | GrÃ¶ÃŸere Modelle brauchen kleinere LR |

### Empirische BegrÃ¼ndung (fÃ¼r Paper)

**Beobachtung:** M1 ohne Dropout (archiv/version_0) hatte bessere Test-Metriken als mit Dropout:

| Konfiguration | Test Accuracy | Test RÂ² | val_loss |
|---------------|---------------|---------|----------|
| Ohne Dropout | **82.5%** | **0.860** | 0.0017 |
| Mit Dropout 0.2 | ~80.5% | ~0.843 | 0.00188 |

**Analyse der Loss-Kurven:** Train- und Val-Loss liefen parallel â†’ kein Overfitting.
Der vermeintliche "Einbruch" war durch zu hohe LR (0.005) verursacht, nicht durch Overfitting.

**Schlussfolgerung:** Dropout verursacht Underfitting bei kleinen Modellen (~85K Parameter).

### Paper-Formulierung (Entwurf)

> "Hyperparameters were tuned per model architecture. Analysis of validation curves
> showed that small models (M1, M2, ~85K parameters) exhibited no overfitting tendency,
> with training and validation loss curves remaining parallel throughout training.
> Therefore, dropout regularization was omitted for these models. Larger models
> (M3-M6, ~600K parameters) used dropout=0.2 to prevent overfitting."

### TODO fÃ¼r Paper: Ablation Study

FÃ¼r Reviewer-Fragen eine Tabelle vorbereiten:

| Model | Dropout | Accuracy | RÂ² | Bemerkung |
|-------|---------|----------|-----|-----------|
| M1 | 0.0 | 82.5% | 0.86 | Best |
| M1 | 0.2 | 80.5% | 0.84 | Underfitting |
| M3 | 0.0 | ? | ? | TODO: testen |
| M3 | 0.2 | ? | ? | TODO: testen |

## Kommandos

### Training starten
```bash
python scripts/train_model.py --config config/model_configs/m1_small_baseline.yaml
python scripts/train_model.py --config config/model_configs/m2_small_simple_attn.yaml
# etc.
```

### Evaluation
```bash
python scripts/evaluate_model.py \
    --checkpoint lightning_logs/M1_Small_Baseline/version_0/checkpoints/best.ckpt \
    --config config/model_configs/m1_small_baseline.yaml \
    --output results/m1_results.json
```

### Vergleichstabelle
```bash
python scripts/compare_results.py results/*.json --output docs/reports/comparison.md
```

## NÃ¤chste Schritte

1. [ ] M1 Training abwarten
2. [ ] M1 evaluieren
3. [ ] M2-M6 nacheinander trainieren
4. [ ] Ergebnisse vergleichen
