# Paper Revision TODO

> **Ziel:** Umstellung des Paper-Fokus von Trainingszeit auf Inferenzzeit
> 
> **Datei:** `/mnt/project/paper.tex`
> 
> **Stand:** 2026-01-26

---

## 칖bersicht der 츿nderungen

| Bereich | 츿nderungstyp | Priorit칛t |
|---------|--------------|-----------|
| Abstract | Text ersetzen | 游댮 Hoch |
| Contributions | Text ersetzen | 游댮 Hoch |
| Hardware-Angabe | Korrektur | 游리 Mittel |
| Metriken (IV.D) | Entfernen + Hinzuf칲gen | 游댮 Hoch |
| Table I | Komplett neu | 游댮 Hoch |
| Section V.A-B | Werte aktualisieren | 游댮 Hoch |
| Section V.C | Komplett umschreiben | 游댮 Hoch |
| Section V.D | Entfernen | 游리 Mittel |
| Section V.E | Neu erstellen | 游리 Mittel |
| Figure 4 | Komplett neu | 游댮 Hoch |
| Figure 5 | Erweitern | 游리 Mittel |
| Conclusion | Aktualisieren | 游댮 Hoch |

---

## Aktuelle Experimentelle Ergebnisse (Referenz)

Diese Werte aus `model_evaluation_results.md` sind die Ground Truth:

```
| Model                      | Params  | Accuracy | R    | RMSE   | Inference P95 |
|----------------------------|---------|----------|-------|--------|---------------|
| M1 Small Baseline          | 84,801  | 82.57%   | 0.860 | 0.0408 | 1.11 ms       |
| M2 Small + Simple Attn     | 84,866  | 81.50%   | 0.850 | 0.0423 | 1.16 ms       |
| M3 Medium Baseline         | 597,633 | 87.84%   | 0.905 | 0.0338 | 2.40 ms       |
| M4 Medium + Simple Attn    | 597,762 | 90.25%   | 0.919 | 0.0311 | 2.44 ms       |
| M5 Medium + Additive Attn  | 630,529 | 88.34%   | 0.907 | 0.0332 | 2.88 ms       |
| M6 Medium + Scaled DP      | 597,633 | 88.17%   | 0.907 | 0.0334 | 2.46 ms       |
```

Dropout-Ergebnisse aus `model_evaluation_results_dropout.md`:

```
| Model                      | No Dropout | Dropout=0.2 | 풊 Accuracy |
|----------------------------|------------|-------------|------------|
| M1 Small Baseline          | 82.57%     | 80.49%      | -2.08%     |
| M2 Small + Simple Attn     | 81.50%     | 80.07%      | -1.43%     |
| M3 Medium Baseline         | 87.84%     | 86.29%      | -1.55%     |
| M4 Medium + Simple Attn    | 90.25%     | 84.31%      | -5.94%     |
| M5 Medium + Additive Attn  | 88.34%     | 85.39%      | -2.95%     |
| M6 Medium + Scaled DP      | 88.17%     | 85.23%      | -2.94%     |
```

---

## 1. Abstract

### 1.1 Trainingszeit-Aussage ersetzen

- [ ] **Zeile ~12-13 in Abstract**

**Suchen:**
```latex
showing that simpler model structures achieve comparable performance with significantly reduced training time (\qty{79}{\percent} reduction).
```

**Ersetzen durch:**
```latex
demonstrating that all proposed models achieve inference times below \qty{3}{\milli\second} on CPU, enabling real-time deployment at over \qty{100}{\hertz} for resource-constrained automotive embedded systems.
```

### 1.2 Accuracy-Wert korrigieren

- [ ] **Zeile ~8 in Abstract**

**Suchen:**
```latex
Simple Attention achieves the highest prediction accuracy of \qty{89.71}{\percent}, outperforming the baseline LSTM (\qty{84.66}{\percent}) by approximately \qty{5}{\percent}.
```

**Ersetzen durch:**
```latex
Simple Attention achieves the highest prediction accuracy of \qty{90.25}{\percent}, outperforming the baseline LSTM (\qty{87.84}{\percent}) by \qty{2.41}{\percent}.
```

---

## 2. Section I: Introduction

### 2.1 Contribution 3 aktualisieren

- [ ] **Im itemize-Block der Contributions**

**Suchen:**
```latex
\item We provide a comprehensive analysis of the trade-off between model complexity and prediction accuracy, showing that simpler models can achieve comparable performance with significantly reduced training time.
```

**Ersetzen durch:**
```latex
\item We provide a comprehensive analysis of the trade-off between model complexity, prediction accuracy, and inference efficiency, demonstrating that attention-enhanced models achieve superior accuracy while maintaining real-time capability ($< \qty{3}{\milli\second}$ inference on CPU) suitable for automotive embedded systems.
```

---

## 3. Section IV: Experimental Setup

### 3.1 Hardware korrigieren (Section IV.C)

- [ ] **In "Implementation Details"**

**Suchen:**
```latex
NVIDIA GeForce RTX 3070 Ti GPU with 8~GB VRAM
```

**Ersetzen durch:**
```latex
NVIDIA GeForce RTX 2060 Super GPU with 8~GB VRAM
```

### 3.2 Metriken entfernen (Section IV.D)

- [ ] **TCR-Definition entfernen**

**Entfernen (komplett):**
```latex
\textbf{Time Cost Ratio (TCR):} The ratio of training time for a complex model to reach the same accuracy as a simpler model:
\begin{equation}
    \text{TCR} = \frac{T_{\text{complex}}}{T_{\text{simple}}}
    \label{eq:tcr}
\end{equation}
```

- [ ] **LE-Definition entfernen**

**Entfernen (komplett):**
```latex
\textbf{Learning Efficiency (LE):} The rate of accuracy improvement per hour:
\begin{equation}
    \text{LE} = \frac{\text{Acc}_{t_2} - \text{Acc}_{t_1}}{t_2 - t_1}
    \label{eq:le}
\end{equation}
```

### 3.3 Neue Metriken hinzuf칲gen (Section IV.D)

- [ ] **Nach RMSE-Definition einf칲gen (beide Metriken):**

```latex
\textbf{Inference Time:} The 95th percentile of single-sample prediction time on CPU, ensuring consistent real-time performance:
\begin{equation}
    T_{\text{inf}} = P_{95}(t_1, t_2, \ldots, t_n)
    \label{eq:inference}
\end{equation}

\textbf{Parameter Efficiency (PE):} The ratio of prediction accuracy to model size, measuring how effectively parameters are utilized:
\begin{equation}
    \text{PE} = \frac{\text{Accuracy}}{|\theta| / 10^5}
    \label{eq:param_efficiency}
\end{equation}
where $|\theta|$ denotes the total number of trainable parameters. Higher PE indicates better utilization of model capacity.
```

---

## 4. Section V.A: Baseline LSTM Performance

### 4.1 Werte aktualisieren

- [ ] **Ersten Absatz anpassen**

**Suchen:**
```latex
The baseline LSTM model with hidden size 64 and 3 layers achieves \qty{84.66}{\percent} accuracy and RMSE of 0.00116 on the test set.
```

**Ersetzen durch:**
```latex
The small baseline LSTM model (M1) with hidden size 64 and 3 layers achieves \qty{82.57}{\percent} accuracy and RMSE of 0.0408 on the test set.
```

- [ ] **Zweiten Absatz anpassen**

**Suchen:**
```latex
Increasing model complexity (hidden size 128, 5 layers) yields only marginal improvement to \qty{86.75}{\percent}, despite a 7-fold increase in model parameters (from 0.339~MB to 2.391~MB).
```

**Ersetzen durch:**
```latex
Increasing model complexity to the medium configuration (M3, hidden size 128, 5 layers) improves accuracy to \qty{87.84}{\percent}, with a 7-fold increase in model parameters (from 85K to 598K).
```

---

## 5. Section V.B: Impact of Attention Mechanisms

### 5.1 Table I komplett ersetzen

- [ ] **Gesamte Tabelle ersetzen**

**Suchen (gesamte Tabelle):**
```latex
\begin{table}[t]
    \centering
    \caption{Performance Comparison of LSTM Models with Different Attention Mechanisms}
    \label{tab:attention_comparison}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Model} & \textbf{Accuracy (\%)} & \textbf{RMSE} \\
        \midrule
        Baseline LSTM & 84.66 & 0.00116 \\
        LSTM + Simple Attention & \textbf{89.71} & \textbf{0.00098} \\
        LSTM + Additive Attention & 88.82 & 0.00101 \\
        LSTM + Scaled Dot-Product Attention & 88.65 & 0.00101 \\
        \bottomrule
    \end{tabular}
\end{table}
```

**Ersetzen durch:**
```latex
\begin{table}[t]
    \centering
    \caption{Performance Comparison of LSTM Models with Different Attention Mechanisms}
    \label{tab:attention_comparison}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Model} & \textbf{Params} & \textbf{Acc. (\%)} & \textbf{RMSE} & \textbf{Inf. (ms)} \\
        \midrule
        M1 Small Baseline & 85K & 82.57 & 0.0408 & 1.11 \\
        M2 Small + Simple Attn & 85K & 81.50 & 0.0423 & 1.16 \\
        \midrule
        M3 Medium Baseline & 598K & 87.84 & 0.0338 & 2.40 \\
        \textbf{M4 Medium + Simple Attn} & 598K & \textbf{90.25} & \textbf{0.0311} & 2.44 \\
        M5 Medium + Additive Attn & 631K & 88.34 & 0.0332 & 2.88 \\
        M6 Medium + Scaled DP Attn & 598K & 88.17 & 0.0334 & 2.46 \\
        \bottomrule
    \end{tabular}
\end{table}
```

### 5.2 Text nach Tabelle aktualisieren

- [ ] **Absatz nach Table I**

**Suchen:**
```latex
The LSTM with Simple Attention achieves the highest accuracy (\qty{89.71}{\percent}) and lowest RMSE (0.00098).
```

**Ersetzen durch:**
```latex
The medium LSTM with Simple Attention (M4) achieves the highest accuracy (\qty{90.25}{\percent}) and lowest RMSE (0.0311). Notably, attention mechanisms do not improve performance for small models: M2 (\qty{81.50}{\percent}) underperforms the baseline M1 (\qty{82.57}{\percent}), suggesting that small architectures lack sufficient representational capacity to benefit from attention weighting.
```

---

## 6. Section V.C: Model Complexity Analysis

### 6.0 NEUE TABLE: Model Efficiency Comparison

- [ ] **Neue Tabelle in Section V.C einf칲gen (zentrale Ergebnistabelle)**

```latex
\begin{table}[t]
    \centering
    \caption{Model Efficiency Comparison: Accuracy, Inference Time, and Parameter Efficiency}
    \label{tab:efficiency}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Model} & \textbf{Params} & \textbf{Acc. (\%)} & \textbf{RMSE} & \textbf{Inf. (ms)} & \textbf{PE} \\
        \midrule
        M1 Small Baseline & 85K & 82.57 & 0.0408 & 1.11 & 97.1 \\
        M2 Small + Simple & 85K & 81.50 & 0.0423 & 1.16 & 96.0 \\
        \midrule
        M3 Medium Baseline & 598K & 87.84 & 0.0338 & 2.40 & 14.7 \\
        \textbf{M4 Medium + Simple} & 598K & \textbf{90.25} & \textbf{0.0311} & 2.44 & \textbf{15.1} \\
        M5 Medium + Additive & 631K & 88.34 & 0.0332 & 2.88 & 14.0 \\
        M6 Medium + Scaled DP & 598K & 88.17 & 0.0334 & 2.46 & 14.8 \\
        \bottomrule
    \end{tabular}
\end{table}
```

**Begleitender Text:**
```latex
\cref{tab:efficiency} summarizes the performance and efficiency metrics for all six model configurations. Parameter Efficiency (PE) measures accuracy per 100K parameters---higher values indicate better utilization of model capacity. While small models achieve significantly higher PE (97.1 vs. 15.1), this comes at the cost of approximately 8 percentage points in accuracy. Among medium models, M4 with Simple Attention achieves both the highest accuracy and the highest PE, demonstrating that attention mechanisms improve parameter utilization rather than merely adding complexity.
```

### 6.1 NEUER SCATTERPLOT: Accuracy vs. Inference Time (Figure 4)

- [ ] **Figure 4 komplett neu erstellen**

```latex
\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            width=0.95\columnwidth,
            height=5.5cm,
            xlabel={Inference Time P95 (ms)},
            ylabel={Accuracy (\%)},
            xmin=0.8, xmax=3.2,
            ymin=80, ymax=92,
            grid=major,
            grid style={dashed, gray!30},
            legend pos=south east,
            legend style={font=\scriptsize, cells={anchor=west}},
            every axis plot/.append style={thick},
        ]
        
        % Small Models (blue circles)
        \addplot[
            only marks, 
            mark=o, 
            mark size=4pt, 
            color=matlabBlue,
            fill=matlabBlue!30
        ] coordinates {
            (1.11, 82.57)
            (1.16, 81.50)
        };
        \addlegendentry{Small (85K params)}
        
        % Medium Baseline (green square)
        \addplot[
            only marks, 
            mark=square*, 
            mark size=4pt, 
            color=matlabGreen
        ] coordinates {
            (2.40, 87.84)
        };
        \addlegendentry{Medium Baseline (598K)}
        
        % Medium + Attention (orange triangles)
        \addplot[
            only marks, 
            mark=triangle*, 
            mark size=5pt, 
            color=matlabOrange
        ] coordinates {
            (2.44, 90.25)
            (2.88, 88.34)
            (2.46, 88.17)
        };
        \addlegendentry{Medium + Attention}
        
        % Model Labels
        \node[font=\scriptsize, anchor=south west] at (axis cs:1.11,82.57) {M1};
        \node[font=\scriptsize, anchor=north west] at (axis cs:1.16,81.50) {M2};
        \node[font=\scriptsize, anchor=east] at (axis cs:2.35,87.84) {M3};
        \node[font=\scriptsize, anchor=south] at (axis cs:2.44,90.45) {\textbf{M4}};
        \node[font=\scriptsize, anchor=west] at (axis cs:2.93,88.34) {M5};
        \node[font=\scriptsize, anchor=north] at (axis cs:2.46,87.97) {M6};
        
        % Highlight best model with circle
        \draw[matlabOrange, thick, dashed] (axis cs:2.44,90.25) circle[radius=8pt];
        
        \end{axis}
    \end{tikzpicture}
    \caption{Trade-off between prediction accuracy and inference time for all model configurations. M4 (Medium + Simple Attention) achieves the highest accuracy (\qty{90.25}{\percent}) with minimal latency overhead (\qty{+1.7}{\percent}) compared to the baseline M3. All models satisfy the \qty{<10}{\milli\second} real-time constraint.}
    \label{fig:inference_tradeoff}
\end{figure}
```

**Begleitender Text:**
```latex
\cref{fig:inference_tradeoff} visualizes the accuracy-latency trade-off across all model configurations. Three key observations emerge: (1) Small models cluster in the lower-left region, offering fast inference but limited accuracy; (2) Medium models with attention consistently outperform the baseline while maintaining similar inference times; (3) M4 dominates the Pareto frontier, achieving the highest accuracy with only \qty{1.7}{\percent} additional latency compared to M3.
```

### 6.2 Komplette Section V.C ersetzen

- [ ] **Gesamte Section V.C ersetzen**

**Suchen (von `\subsection{Model Complexity Analysis}` bis zur n칛chsten `\subsection`):**
- Alles inkl. Table II (Simple vs Complex)
- Alles inkl. Table III (TCR Analysis)  
- Alles inkl. Figure 4 (alte Version)

**Ersetzen durch:**
```latex
\subsection{Inference Efficiency Analysis}

For deployment in automotive embedded systems, inference time is critical. \cref{tab:inference} summarizes the computational efficiency of all models.

\begin{table}[t]
    \centering
    \caption{Inference Efficiency on CPU (Intel Core i7)}
    \label{tab:inference}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Model} & \textbf{Inf. P95 (ms)} & \textbf{Max Hz} & \textbf{Attn. Overhead} \\
        \midrule
        M1 Small Baseline & 1.11 & 900 & -- \\
        M2 Small + Simple & 1.16 & 862 & +4.5\% \\
        \midrule
        M3 Medium Baseline & 2.40 & 417 & -- \\
        M4 Medium + Simple & 2.44 & 410 & +1.7\% \\
        M5 Medium + Additive & 2.88 & 347 & +20.0\% \\
        M6 Medium + Scaled DP & 2.46 & 407 & +2.5\% \\
        \bottomrule
    \end{tabular}
\end{table}

All models comfortably exceed the \qty{100}{\hertz} real-time requirement for EPS systems. The computational overhead of attention mechanisms varies significantly: Simple Attention adds only \qty{1.7}{\percent} latency while providing the largest accuracy gain (\qty{+2.41}{\percent}), whereas Additive Attention increases latency by \qty{20}{\percent} for a smaller accuracy improvement (\qty{+0.50}{\percent}).

\cref{fig:inference_tradeoff} visualizes the accuracy-latency trade-off across all models.

%%% NEW Figure 4: Accuracy vs Inference Time
\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            width=0.95\columnwidth,
            height=5.5cm,
            xlabel={Inference Time P95 (ms)},
            ylabel={Accuracy (\%)},
            xmin=0.8, xmax=3.2,
            ymin=80, ymax=92,
            grid=major,
            grid style={dashed, gray!30},
            legend pos=south east,
            legend style={font=\scriptsize},
        ]
        
        % Small Models
        \addplot[only marks, mark=o, mark size=3pt, color=matlabBlue] 
            coordinates {(1.11, 82.57) (1.16, 81.50)};
        \addlegendentry{Small (85K)}
        
        % Medium Baseline
        \addplot[only marks, mark=square*, mark size=3pt, color=matlabGreen] 
            coordinates {(2.40, 87.84)};
        \addlegendentry{Medium Baseline}
        
        % Medium + Attention
        \addplot[only marks, mark=triangle*, mark size=4pt, color=matlabOrange] 
            coordinates {(2.44, 90.25) (2.88, 88.34) (2.46, 88.17)};
        \addlegendentry{Medium + Attention}
        
        % Labels
        \node[font=\scriptsize, anchor=south] at (axis cs:1.11,82.57) {M1};
        \node[font=\scriptsize, anchor=north] at (axis cs:1.16,81.50) {M2};
        \node[font=\scriptsize, anchor=east] at (axis cs:2.40,87.84) {M3};
        \node[font=\scriptsize, anchor=south] at (axis cs:2.44,90.25) {M4};
        \node[font=\scriptsize, anchor=west] at (axis cs:2.88,88.34) {M5};
        \node[font=\scriptsize, anchor=north] at (axis cs:2.46,88.17) {M6};
        
        \end{axis}
    \end{tikzpicture}
    \caption{Trade-off between prediction accuracy and inference time. M4 (Simple Attention) achieves the highest accuracy with minimal latency overhead.}
    \label{fig:inference_tradeoff}
\end{figure}

The choice between small and medium models presents a clear trade-off: small models offer approximately \qty{2}{\times} faster inference (\qty{\sim 1}{\milli\second} vs. \qty{\sim 2.5}{\milli\second}) but sacrifice approximately 8 percentage points in accuracy.
```

---

## 7. Section V.D: Window Size - ENTFERNEN

- [ ] **Gesamte Section V.D entfernen**

**Entfernen (komplett):**
```latex
\subsection{Impact of Window Size}

The window size significantly affects model performance, as shown in \cref{tab:window_size}.
Increasing the window size from 15 to 50 time steps improves accuracy by approximately \qty{5}{\percent}.

%%% Table: Window Size
\begin{table}[t]
    \centering
    \caption{Impact of Window Size on Model Performance}
    \label{tab:window_size}
    \begin{tabular}{ccc}
        \toprule
        \textbf{Window Size} & \textbf{Accuracy (\%)} & \textbf{RMSE} \\
        \midrule
        15 (1.5s) & 83.86 & 0.00119 \\
        50 (5.0s) & 88.72 & 0.00104 \\
        \bottomrule
    \end{tabular}
\end{table}

Larger windows capture more temporal context, enabling the model to learn longer-term dependencies in vehicle dynamics.
At 10~Hz sampling frequency, a window of 50 time steps corresponds to 5 seconds of driving data, which provides sufficient context for predicting steering torque requirements.
```

---

## 8. Section V.E: Dropout Ablation - NEU ERSTELLEN

- [ ] **Neue Section nach V.C (bzw. alter V.D Position) einf칲gen**

**Einf칲gen:**
```latex
\subsection{Effect of Dropout Regularization}

To investigate whether dropout regularization could improve generalization, we trained all model variants with dropout probability $p=0.2$ applied to LSTM layers. \cref{tab:dropout} presents the results.

\begin{table}[t]
    \centering
    \caption{Effect of Dropout ($p=0.2$) on Model Performance}
    \label{tab:dropout}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Model} & \textbf{No Dropout} & \textbf{Dropout} & \textbf{$\Delta$} \\
        \midrule
        M1 Small Baseline & 82.57\% & 80.49\% & -2.08\% \\
        M2 Small + Simple & 81.50\% & 80.07\% & -1.43\% \\
        M3 Medium Baseline & 87.84\% & 86.29\% & -1.55\% \\
        M4 Medium + Simple & 90.25\% & 84.31\% & \textbf{-5.94\%} \\
        M5 Medium + Additive & 88.34\% & 85.39\% & -2.95\% \\
        M6 Medium + Scaled DP & 88.17\% & 85.23\% & -2.94\% \\
        \bottomrule
    \end{tabular}
\end{table}

Dropout consistently degraded performance across all models. Attention-augmented models were particularly affected, with M4 suffering the largest drop of \qty{5.94}{\percent}. This suggests that the dataset of 2.2 million samples provides sufficient regularization through data diversity, and that attention mechanisms are sensitive to noise introduced by dropout in intermediate representations. All subsequent results use no dropout.
```

---

## 9. Section V: Attention Weight Analysis

### 9.1 Figure 5 erweitern (Optional - niedrigere Priorit칛t)

- [ ] **Figure 5 um Additive und Scaled DP erweitern**

**Hinweis:** Erfordert Attention-Weight-Daten aus:
- `results/figures/M4_Medium_Simple_Attention/M4_Medium_Simple_Attention_attention_weights.npy`
- `results/figures/M5_Medium_Additive_Attention/M5_Medium_Additive_Attention_attention_weights.npy`
- `results/figures/M6_Medium_Scaled_DP_Attention/M6_Medium_Scaled_DP_Attention_attention_weights.npy`

**TODO:** Pr칲fen ob diese Dateien verf칲gbar sind, dann 3-Panel-Figure erstellen.

---

## 10. Section VI: Conclusion

### 10.1 Accuracy-Werte korrigieren

- [ ] **Im ersten Bullet Point**

**Suchen:**
```latex
\item The LSTM model with Simple Attention achieves the highest prediction accuracy (\qty{89.71}{\percent}), outperforming the baseline LSTM by \qty{5}{\percent} and more complex attention mechanisms by approximately \qty{1}{\percent}.
```

**Ersetzen durch:**
```latex
\item The LSTM model with Simple Attention (M4) achieves the highest prediction accuracy (\qty{90.25}{\percent}), outperforming the medium baseline by \qty{2.41}{\percent} and more complex attention mechanisms by approximately \qty{2}{\percent}.
```

### 10.2 Trainingszeit-Aussage ersetzen

- [ ] **Im zweiten Bullet Point**

**Suchen:**
```latex
\item Simpler model architectures achieve comparable performance with significantly reduced computational cost, with the simple model demonstrating 4$\times$ higher learning efficiency than the complex model.
```

**Ersetzen durch:**
```latex
\item All models achieve inference times below \qty{3}{\milli\second} on CPU, with Simple Attention adding only \qty{1.7}{\percent} latency overhead while providing the largest accuracy improvement, making it suitable for real-time deployment at over \qty{100}{\hertz}.
```

### 10.3 Future Work erweitern

- [ ] **In der Future Work Liste**

**Suchen:**
```latex
Future work will explore: (1) larger window sizes with memory-efficient implementations,
```

**Ersetzen durch:**
```latex
Future work will explore: (1) the impact of sliding window size on the accuracy-latency trade-off for memory-constrained systems,
```

---

## 11. Sonstige Korrekturen

### 11.1 TODO-Kommentar im Text entfernen

- [ ] **In Section I**

**Suchen und entfernen:**
```latex
% TODO: Replace reference - Xing et al. 2019 is about driver activity recognition, NOT steering angle prediction
```

### 11.2 Figure 3 Entscheidung

- [ ] **ENTSCHEIDUNG TREFFEN:** Figure 3 (Training Curves) behalten oder entfernen?

**Option A - Behalten:** 
- Zeigt Konvergenzverhalten
- Weniger prominent im Text erw칛hnen

**Option B - Entfernen:**
- Nicht zentral f칲r neues Narrativ
- Spart Platz

**Empfehlung:** Behalten, aber Text k칲rzen.

---

## Checkliste f칲r Review

Nach allen 츿nderungen pr칲fen:

- [ ] Alle Accuracy-Werte konsistent (90.25% f칲r M4, 87.84% f칲r M3, etc.)
- [ ] Alle RMSE-Werte im neuen Format (~0.03-0.04, nicht ~0.001)
- [ ] Keine Referenzen auf TCR oder Learning Efficiency mehr
- [ ] Keine Referenzen auf Trainingszeit als Hauptmetrik
- [ ] Figure-Nummern nach Entfernung/Einf칲gen noch korrekt
- [ ] Table-Nummern nach Entfernung/Einf칲gen noch korrekt
- [ ] `\cref{}` Referenzen alle g칲ltig
- [ ] Hardware: RTX 2060 Super (nicht 3070 Ti)
- [ ] Window Size nur in Future Work erw칛hnt

---

## Dateien

| Datei | Beschreibung |
|-------|--------------|
| `/mnt/project/paper.tex` | Hauptdatei (READ-ONLY, kopieren nach /home/claude/) |
| `/mnt/project/model_evaluation_results.md` | Aktuelle Ergebnisse (Referenz) |
| `/mnt/project/model_evaluation_results_dropout.md` | Dropout-Ablation (Referenz) |

---

## Ausf칲hrungsreihenfolge

1. **Kopiere** `paper.tex` nach `/home/claude/paper.tex`
2. **F칲hre 츿nderungen durch** in der Reihenfolge dieser TODO-Liste
3. **Kompiliere** LaTeX zur Validierung: `pdflatex paper.tex`
4. **Pr칲fe** Referenzen und Nummerierungen
5. **Kopiere** fertiges Paper nach `/mnt/user-data/outputs/`

---

*Erstellt: 2026-01-26*